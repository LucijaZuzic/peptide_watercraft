%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%%%\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

\let\LaTeXcline\cline\documentclass[sn-mathphys-num]{sn-jnl}\let\cline\LaTeXcline

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
\usepackage{anyfontsize}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[aaa]

\author*[1,2]{\fnm{Lucija} \sur{\v{Z}u\v{z}i\'{c}}}\email{lucija.zuzic@uniri.hr}
\equalcont{These authors contributed equally to this work.}

\author[1,2,3]{\fnm{Renato} \sur{Filjar}}\email{renato.filjar@uniri.hr}

\affil*[1]{\orgdiv{Department of Computer Engineering}, \orgname{Faculty of Engineering, University of Rijeka}, \orgaddress{\street{Vukovarska 58}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[2]{\orgdiv{Center for Artificial Intelligence and Cybersecurity}, \orgname{University of Rijeka}, \orgaddress{\street{Radmile Matejcic 2}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[3]{\orgdiv{Laboratory for Spatial Intelligence}, \orgname{Hrvatsko Zagorje Krapina University of Applied Sciences}, \orgaddress{\street{Setaliste hrvatskog narodnog preporoda 6}, \city{Krapina}, \postcode{49000}, \country{Croatia}}}

\abstract{ }

\keywords{ }

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
%\label{sec:Intro}

The problem addressed in this paper is the creation of a classifier of aircraft trajectories based on the temporal and geometric features of the trajectories and meteorological features. Other works have successfully used the \textit{R} \textit{trajr} library \cite{McLean2018-jr} to analyze the trajectories of animals, such as griffon vultures \cite{dumenvcic2022trajectory}, and vehicles, such as airplanes \cite{Makar2022, filjarairplane}. Aircraft flight data is publicly available thanks to OpenSkyNetwork, a non-profit network of Automatic Dependent Surveillance-Broadcast (ADS-B) receivers, and has been studied for aircraft trajectory analysis \cite{Makar2022}. 

Publicly available data on meteorological features was also used to describe the trajectories. Meteorological conditions influence the frequency of incidents, as confirmed by reports on airplane accidents \cite{Gultepe2019}. The results of trajectory classification based on features provided by the \textit{trajr} library, meteorological features, and both sets of features were compared. To more easily adapt the class decision process for devices with lower computational capabilities, a model was built that uses only two features that encompass both static (scalar-based) and dynamic (vector-based) aspects of the analysis provided by the \textit{trajr} library.

Before model training, the trajectories were separated into two classes by visual inspection. The departure airport Zagreb Pleso, and the destination airport London Heathrow were selected. A regular airline connects these airports on Mondays, so the regular schedule and limited scope simplify the experiment. In addition to the periodically recurring connection, the ease of analysis is facilitated by the fact that there is only one takeoff runway from Zagreb Pleso Airport. It is not necessary to separate trajectories based on multiple runways but on the direction of movement along the single existing runway.

This work hypothesizes that based on the features of the aircraft trajectory obtained using the \textit{trajr} library and meteorological features, it is possible to classify previously marked trajectories in R without directly entering the coordinates into the model.

The k-Nearest Neighbor (k-NN), Linear and Radial Basis Function (RBF) Support Vector Machine (SVM), Gaussian Process (GP), Decision Tree (DT), Random Forest (RF), Multilayer Perceptron (MLP), Naive Bayes (NB), Quadratic Discriminant Analysis (QDA) and AdaBoost (AB) algorithms were used for classification in this paper. The listed classifiers have solved various problems, including individual mobility classification \cite{csl-202004-0019} and behavior analysis \cite{Garcia_Ceja2021-tw}. Bosson and Nikoleris \cite{Bosson2018} use some of these classifiers and additional methods to predict the runway on which airplanes land, while this paper focuses on takeoff direction.

The hypothesis is that temporal and geometric features of trajectories, which have already been used in other methods of trajectory analysis \cite{dumenvcic2022trajectory, Makar2022}, have a greater influence on the class of trajectories than meteorological conditions, so classifiers that use them will be more successful.
 
The hypothesis also claims that among the meteorological features at the selected departure airport Zagreb Pleso, temperature and dew point have the greatest influence on aircraft trajectories \cite{Gultepe2019}. At low temperatures, ice can suddenly form on aircraft wings, which seriously impairs their aerodynamic properties and causes sudden descent and an inability to fly. Icing occurs due to the atmospheric physical properties. Water droplets exist at temperatures lower than $0$ degrees $\mathrm{C}$, but freeze rapidly when they come in contact with an aircraft, its wings or engine, and other solid surfaces. Aircraft icing can be extremely dangerous. Modern aircraft have anti-icing systems, but icing can still be a threat, especially for light aircraft. Meteorologists forecast icing zones and forward such information to their users to prevent accidents.

The AdaBoost algorithm, which uses Decision Trees as weak classifiers, is often called the best classifier using initial settings without adjustment \cite{Kegl2013}, so it is hypothesized to be more successful than other classifiers.

\section{Method and data}
\label{sec:Dataset}

A k-Nearest Neighbours (k-NN), Linear and Radial Basis Function (RBF) Support Vector Machine (SVM), Gaussian Process (GP), Decision Tree (DT), Random Forest (RF), Naive Bayes (NB), Multilayer Perceptron (MLP), AdaBoost (AB), and Quadratic Discriminant Analysis (QDA) method were tested based on their ability to classify a set of observations of trajectory features, and other meteorological predictors, into one of the scenarios of trajectory classes based on the first three points of a trajectory and their position when it comes to the origin airport. Two classes were predefined using observed data on a map. Statistical analysis of the data confirmed that distributions of trajectory features change for different labels, supporting the validity of the labeling. The study assumes that the dependent output variable, the trajectory class, can be predicted based on the independent variables used as input. Meteorological data was obtained using the rp5.ru website. The OpenSky Network database contains state vectors transmitted for an aircraft. The two datasets are merged based on location, date, and time.

\subsection{Method}
%\label{subsec:Method}

The methods were selected because they represent larger families of classification methods. Support Vector Machine (SVM) models are supervised maximum margin models. Decision Tree (DT) models also apply supervised learning based on decision rules, while Random Forest (RF) and AdaBoost (AB) algorithms can utilize multiple DT classifies. The k-Nearest Neighbours algorithm (k-NN) is a non-parametric supervised learning classification method using votes from the nearest data points. Naive Bayes (NB) classifiers are probabilistic classifiers that can be parametric or non-parametric, but this study uses a non-parametric approach. A Gaussian Process (GP) is another parametric model that can be combined with Bayesian inference. A shallow neural network (NN) approach to machine learning (ML) is represented by the Multilayer Perceptron (MLP) method. Artificial intelligence (AI) techniques replicate biological neurons using layers. Input is processed using an activation function and the result is used for the next layer until the final output. Feedforward methods such as MLP do this only in one direction, with standard backpropagation for training. Quadratic Discriminant Analysis (QDA) uses a non-parametric non-linear decision surface to create a classification.

\subsubsection{K-Nearest Neighbours}
%\label{subsubsec:k-NN}

In statistics, the k-Nearest Neighbours algorithm (k-NN) is a non-parametric supervised learning classification and regression method first developed by Evelyn Fix and Joseph Hodges in 1951 \cite{Fix1989} and later extended by Thomas Cover \cite{Cover1967}. An object is classified by the majority vote of its $k$ nearest neighbors.

Since this algorithm relies on distance for classification, if the features represent different physical units or have very different scales, normalizing the data and removing noisy or unimportant features before training can dramatically improve accuracy \cite{Hastie2009}.
 
The best choice of $k$ depends on the data. Larger values of $k$ reduce the effect of noise \cite{Everitt2011}, but make the boundaries between classes less clear. A good $k$ can be chosen via the bootstrap method \cite{Hall2008} or using different heuristic techniques, such as hyperparameter optimization. In binary classification, $k$ is often an odd number to avoid ties.

The \textit{knn3} function from the \textit{caret} library \cite{Kuhn2007} was applied in this study. The \textit{train} function from the \textit{caret} library \cite{Kuhn2007} was used to set the parameter network for tuning the $k$ value with ten folds for cross-validation and three repetitions. Integers from one to twenty, including the range limits, were tested as $k$ values.

\subsubsection{Gaussian Process}
%\label{subsubsec:GP}

In probability theory and statistics, a Gaussian Process (GP) is a stochastic process, a set of random variables indexed in time or space, such that any finite set of these random variables has a multivariate normal distribution. The distribution of a Gaussian Process is the joint distribution of infinitely many random variables represented by functions with a continuous domain, such as time or space.

Gaussian Processes are useful in statistical modeling because they benefit from properties inherited from the normal distribution. While exact models often scale poorly for large amounts of data, multiple approximation methods retain good accuracy while drastically reducing computation time.
  
A Gaussian Process can be used as a prior probability distribution of functions in Bayesian inference \cite{Rasmussen2005, Liu2010}. Given any set of $N$ points in the desired function domain, one can study a multivariate Gaussian Process whose covariance matrix parameter is the Gram matrix of $N$ points with the desired kernel probability density function and sample from that process.

The \textit{gausspr} function from the \textit{kernlab} library \cite{Karatzoglou2004} is an implementation of the Gaussian Process that can be used for regression or classification and is based on the work written by Williams and Barber \cite{Williams1998}.

\subsubsection{Random Forest}
%\label{subsubsec:RF}

Random Forest (RF) or Random Decision Forest is a collective learning method for classification, regression, and other tasks, constructing many Decision Trees during training. For classification, the output class is chosen by the majority of Decision Trees.

Random Decision Forests remove the tendency of Decision Trees to overfit to the training data set \cite{Ho1995}. The Random Forest algorithm implements a classification approach called "stochastic discrimination" \cite{Kleinberg1990, Kleinberg1996, Kleinberg2000}. 

Leo Breiman \cite{Breiman2001} and Adele Cutler \cite{Breiman2002} combined bagging and random feature selection \cite{Ho1995, Amit1997} to build a collection of Decision Trees with controlled variance and extend the Random Forest method. The implementation of Random Forest in the function \textit{randomForest} in the library of the same name is based on Breiman and Cutler's work \cite{Breiman2001, Breiman2002}.

\subsubsection{Multilayer Perceptron}
%\label{subsubsec:MLP}

Multilayer Perceptron (MLP) is the name for an artificial neural network (ANN) with feedforward data transfer, which consists of fully connected neurons with a nonlinear activation function, organized in at least three layers, known for being able to distinguish data that are not linearly separable \cite{Cybenko1989}. Modern feedforward neural networks are trained using backpropagation for gradient estimation \cite{Linnainmaa1976, Kelley1960, Rosenblatt1962, Werbos1970, Rumelhart1987}. Shallow neural networks with a single hidden layer and standard backpropagation, such as MLP, are colloquially called "vanilla" neural networks \cite{Hastie2009}. The function \textit{MLP} from the \textit{fdm2id} library \cite{Blansche2019} is based on the implementation of a neural network with one layer from the function \textit{nnet} in the library of the same name \cite{Ripley2009}.

\subsubsection{Support Vector Machine}
%\label{subsubsec:SupportVectorMachine}

The Support Vector Machine (SVM) or Support Vector Network (SVN) algorithm is a supervised maximum margin classification approach. Non-linear classification is possible thanks to the hyperplane kernel trick \cite{Boser1992}. The hyperplane furthest from the nearest training sample of any class represents optimal division \cite{HastieRosset2009}. SVM models were pitted against other classification methods in extensive tests \cite{Meyer2003}. Despite this, their performance status regarding various linear models, such as logistic and linear regression, has yet to be established. The selected kernel probability density function is crucial for successful classification \cite{Press2007}. A linear and a Radial Basis Function (RBF) are applied in this experiment. The \textit{svm} function from the \textit{e1071} library \cite{Meyer1999} was used to build the SVM in this work. The implementation in the mentioned function is based on the implementation for C/C++ developed by Chang and  Lin for the \textit{LIBSVM} library \cite{Chang2007}. More details on the implementation and speed benchmarks can be found in the paper by Fan et al. \cite{Fan2008}.

\subsubsection{Decision Tree}
%\label{subsubsec:DecisionTree}

Decision Tree (DT) methods apply supervised classification or regression in data mining and decision-making. The output is discrete for classification trees. DT models are easy to understand, contributing to their widespread use \cite{Wu2008}. Feature-derived rules for tree construction iteratively split the original sample set, or root node, into descendants, or successors containing its subsets \cite{ShalevShwartz2014}. The function \textit{rpart} from the library of the same name \cite{Therneau1999} that was used in this project follows the work of Breiman et al. \cite{Breiman1984}.

\subsubsection{Naive Bayes}
%\label{subsubsec:NaiveBayes}

Naive Bayes (NB) models, simple Bayes or independent Bayes \cite{Hand2001} classifiers are a wide category of linear "probabilistic classifiers". It is hypothesized the predictors are conditionally independent if the output variable is known. NB models apply maximum likelihood training \cite{Russell1999} with linear complexity and no expensive iterations. Boosted Trees or Random Forest (RF) models outperform Naive Bayes (NB) models according to 2006 data \cite{Caruana2006}. However, NB models do not require as much training data as others \cite{John2013}. Using Bayes' theorem in Equation~\ref{eqn:1}, NB models determine probabilities $p(C_{k}\mid x_{1},\ldots, x_{n})$ for $K$ classes $C_{k}$ and an input vector $x = (x_{1},\ldots, x_{n})$ of $n$ predictors \cite{Murty2011}. The \textit{gaussian\_naive\_bayes} function from the \textit{naivebayes} library \cite{Majka2017} was used in this paper to build a Gaussian classifier.

\begin{equation}
	p(C_{k}\mid \mathbf{x})={\frac{p(C_{k})\ p(\mathbf{x} \mid C_{k})}{p(\mathbf{x})}}
	\label{eqn:1}
\end{equation}

\subsubsection{AdaBoost}
%\label{subsubsec:AdaBoost}

AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm. The output of other learning algorithms, or weak classifiers, is combined into a weighted sum. The relative "weight" of each training sample's class prediction is gathered in each iteration.

This data can be fed into a Decision Tree training algorithm. Later trees can use the information to focus on difficult-to-classify samples. Decision trees can be built that favor splitting sample sets with large weight sums.

Each weak learner produces an output hypothesis $h$ that improves the prediction $h(x_{i})$ for each sample in the training set. In each iteration, marked as $t$, a weak classifier is selected and a coefficient $\alpha_{t}$ is determined such that the total training error $E_{t}$ of the resulting $t$-stage boosted classifier, given in Equation~\ref{eqn:2}, is minimized.
 
\begin{equation}
[E_{t} = \sum_i E[F_{t-1}(x_{i}) + \alpha_{t} h(x_{i})] \label{eqn:2}
\end{equation}
 
$F_{t-1}(x)$ is the boosted classifier built in the previous training stage, and $f_{t}(x) = \alpha_{t} h(x)$ is the weak classifier considered to be added. In each iteration of the training process, a weight $w_{i,t}$ equal to the current error $E(F_{t-1}(x_{i}))$ is assigned to all samples. The \textit{adaboost} function from the \textit{JOSUBoost} library \cite{Olson2016}, based on a paper written by Freund and Shapire \cite{Freund1997}, is used in this paper.

\subsubsection{Quadratic Discriminant Analysis}
%\label{subsubsec:QDA}
 
Quadratic Discriminant Analysis (QDA) is a statistical classifier that uses a quadratic decision surface to separate measurements of two or more classes of objects or events. It is a more general version of a linear classifier and closely related to Linear Discriminant Analysis (LDA) \cite{Tharwat2016}. However, unlike LDA, in QDA there is no assumption that the measurements of each class are distributed according to a normal (Gaussian) distribution \cite{Cover1965}. The \textit{qda} function from the \textit{MASS} library \cite{Ripley2009Mass} used in this paper is based on work by Ripley \cite{Ripley1996} and an implementation for the \textit{S} language by Vernables and Ripley \cite{Venables2002}.

\subsection{Data description and analysis}
%\label{subsec:Data}

This paper calculates different temporal and geometric features of the trajectories representing aircraft flights from the selected point of departure to the destination airport using the statistical computing software R. Then, based on the trajectory features obtained using the \textit{trajr} library and meteorological features, obtained from meteorological reports for a certain time and place, the previously marked trajectories are classified without directly using the coordinates as input for the model. Such an analysis should confirm the significance of the temporal and geometric features of the trajectories that distinguish and describe them, and enable performance comparison of different classifiers. The results could also provide insight into the possible influence of different meteorological factors on aircraft movement.

\subsubsection{OpenSkyNetwork}
%\label{subsubsec:opensky}

OpenSkyNetwork is a non-profit network of Automatic Dependent Surveillance-Broadcast (ADS-B) receivers launched in 2013 by the Swiss Defense Office, the University of Kaiserslautern, and the University of Oxford to collect air traffic data for scientific research.

The OpenSky network saves received raw data only for the immediate past due to the excessive amount of such data. Access to real-time data is limited to 8 minutes per day for unregistered users and 90 minutes per day for registered users, and there are other restrictions depending on the type of query.
 
State vector information, which also includes flight trajectories from a given airport, is limited to a maximum of one week in the past if real-time data access is used. A subset of historical processed or unprocessed information is available without registration, while full access requires a subscription and submitting a request. The "states" data set is available to everyone and contains state vectors for each full hour on Mondays between 05.06.2017. to 27.6.2022.

Among the data downloaded in this way, ICAO24 is a unique 24-bit address determined by the International Civil Aviation Organization (ICAO) for each object that transmits ADS-B messages, such as an airport, which is the case in this study. The ICAO24 identifier is written in hexadecimal and stored as a character sequence. The callsign of the aircraft used in communication is also stored as a character string. Callsigns are derived from several ICAO or Federal Aviation Administration (FAA) rules in the United States of America. Both the ICAO24 and callsign values are necessary to identify an aircraft trajectory for a given time interval.

The values of latitude, longitude, and altitude in meters were used to build the trajectory. The timestamps as seconds elapsed since the so-called epoch time (00:00:00, 01.01.1970, UTC) are also necessary for building the trajectory. State vectors with missing values in the \textit{longitude}, \textit{latitude}, or \textit{geoaltitude} fields were omitted during post-processing.

The \textit{openSkies} library provides access to the OpenSky API and the OpenSky historical database, which is published on the CRAN network.

As part of this project, the \textit{getAirportMetadata} function was used to obtain the latitude and longitude of the Zagreb Pleso airport and define the observation radius. An area of $0.4$ degrees longitude and latitude was selected, approximately equal to $44.4$ $km$ at the latitude and longitude of this airport. The observed area is limited to the initial phase of the flight near the airport, and only this part of each trajectory is used in further processing.

The function \textit{getAirportDepartures} was used to retrieve departure flights from a specific airport in the desired time range. In this case, it is necessary to specify the ICAO24 identifier for Zagreb Pleso airport, LDZA, and the beginning and end of the time interval. An object is returned, containing all the flights that match the specified parameters, and the response is filtered by the ICAO24 identifier for the destination airport, London Heathrow, EGLL.

The ICAO24 identifier and the call sign were used to connect flights with state vectors. Only the state vectors with a timestamp between departure and arrival are recorded. The values of the state vector are also associated with meteorological features at the appropriate measuring station by timestamp and location, as described in the rest of the text.

\subsubsection{Meteorological features}
%\label{subsubsec:METAR}

Publicly available meteorological data, obtained from METAR reports, was used along with aircraft trajectory features to train classifiers in this paper using manually marked trajectories.

Meteorological Aerodrome Report (METAR) is a format for reporting meteorological data obtained by observation or sensors. METAR reports are used mainly by aircraft pilots and meteorologists, who use the aggregated information for weather forecasts. This format is highly standardized by ICAO and the World Meteorological Organization (WMO), and understood in most of the world. The website $rp5.ru$ \cite{rp5Description} publishes meteorological forecasts and archives of historical METAR reports in various file formats for numerous measuring stations and airports.

The temperature and dew point were used in this experiment and measured in degrees Celsius. Information about relative humidity between zero and $100$ percent was also used in the analysis. High relative humidity indicates the dew point is closer to the current air temperature. Relative humidity equal to $100\%$ means that the dew point is the same as the current temperature and air is maximally saturated with water. The relative humidity decreases when the dew point remains constant and the temperature rises.

Air pressure values expressed in $mmHg$ (millimeters of mercury) recorded at the altitude of the measuring station and sea level ($P$), were also used. Wind speed in $m/s$ (meters per second) is included in the set of predictors.

Measurements of meteorological features are added every half hour or every full hour, and state vectors for aircraft are stored every ten seconds in analyzed files. The value from the meteorological report with the smallest time gap was used for each aircraft state vector. The arithmetic mean of meteorological features is then calculated to classify a trajectory.

\subsubsection{Trajectory resampling}
%\label{subsubsec:sample}

After removing points with missing latitude, longitude, or altitude values, the function \textit{TrajResampleTime} was used to linearly interpolate points along the trajectory to create a new trajectory with fixed time intervals between points. Since the distance between points was ten seconds in the original state vector data, the same value was used again to avoid data loss and maintain the same file size.
 
\subsubsection{Trajectory smoothing}
%\label{subsubsec:smooth}

Many trajectories include noise that can affect the results of some analyses. The \textit{TrajSmoothSG} function reduces high-frequency noise while preserving the trajectory shape by applying a Savitzky-Golay smoothing filter from the function \textit{sgolayfilt} in the \textit{signal} library \cite{Ligges2006}. The degree of the polynomial used in the filter, and the length of the filter, which must be an odd number, both need to be specified.

High-frequency noise is common in the obtained data, especially in altitude values, so smoothing was applied with the degree of the polynomial $p = 3$ and the window size $n = 11$. The values were chosen because of the ten-second interval between records and verified by visual assessment of the shape of the curve representing the trajectory.

\subsubsection{Speed}
%\label{subsubsec:speed}

The function \textit{TrajDerivatives} calculates the first and second-order derivatives along the trajectory, representing speed and acceleration. The trajectory was smoothed to avoid irregularities before the derivatives were calculated. This paper uses the mean value of the speeds for the trajectory as a predictor variable.

\subsubsection{Straightness}
%\label{subsubsec:straightness}
 
Various methods for measuring the straightness, or the curvature of trajectories, are available within the \textit{trajr} library. The simplest method is calculating $D/L$, where $D$ is the Euclidean distance of the beginning and end points of the trajectory, and $L$ is the length of the trajectory. This approach is implemented in the \textit{TrajStraightness} function. The return value is between $0$ to $1$, with $1$ indicating a straight line with equal Euclidean distance and length, and $0$ representing curves. Benhamou \cite{Benhamou2004} believes that the $D/L$ equation is a reliable measure of the efficiency of a directed walk, but not a random trajectory. The analyzed aircraft trajectories have a defined goal, the destination airport, and the $D/L$ formula is applicable.
 
\subsubsection{Sinuosity}
%\label{subsubsec:sinuosity2}

The sinuosity index defined by Benhamou \cite{Benhamou2004} and used in this experiment is a function of the mean cosine of the turning angles, representing a suitable measure of the curvature of a trajectory representing a random walk. This is a corrected form of the original sinuosity index defined by Bovet and Benhamou \cite{Bovet1988}. The function \textit{TrajSinuosity2} calculates the corrected form of the sinuosity index. The uncorrected form of the curvature index should only be used for a trajectory with a constant step size, which is not the case in this experiment.

The equation for the corrected sinuosity index is $S = 2 {[p(((1+c)/(1-c))+b^{2})]}^{-0.5}$, where $p$ denotes the mean step size, $c$ the mean cosine of the turning angles, and $b$ the step size variation coefficient. A sinuosity index closer to $1$ represents straighter trajectories because the cosine of turning angles approaches $1$ as the turning angle decreases. A sinuosity index near $0$ indicates a curve because the cosine of turning angles close to a right angle approaches $0$.

\subsubsection{Maximum expected displacement}
%\label{subsubsec:emax}

$E_{max}^{a}$ is a dimensionless estimate of the maximum expected displacement from the beginning to the end of the trajectory returned by the function \textit{TrajEmax}. Larger values represent straighter trajectories \cite{Cheung2007}). $E_{max}^{b}$ is $E_{max}^{a}$ multiplied by the mean step size, thus giving the maximum possible displacement expressed in spatial units. The dimensionless $E_{max}^{a}$ value was used in this study because the step size is not constant. The formula for maximum expected displacement is $f * b / (1 - b)$, where $f$ represents the mean value of the step sizes returned by the function \textit{TrajStepLengths} or $1$ for a dimensionless estimated. In the same formula, $b$ represents the mean value of the cosine of the turning angles returned by the function \textit{TrajAngles}.
 
\subsubsection{Fractal dimension}
%\label{subsubsec:fd}

In mathematics, fractal dimension is a term used in geometry to give a rational statistical index of the complexity of detail in a pattern \cite{Mandelbrot1967}. A fractal pattern changes with the scale at which it is measured, which is the step size for trajectories. The fractal dimension is also a measure of the space occupied by the pattern and describes how the fractal scales differently in the non-integer fractal dimension.

The fractal dimension is considered a promising measure of straightness or curvature, varying between $1$ for a straight line and $2$ for Brownian motion. However, several studies have found it unsuitable for animal trajectories, because they are not fractal curves, and the fractal dimension depends on the provided range of step sizes \cite{Nams2006, Turchin1996}.

Fractal dimension is still used, despite these limitations, and the function \textit{TrajFractalDimension} by default calculates the fractal dimension using a modified  "dividers" method to account for rounding errors \cite{Nams2006}. This function compensates for overestimating the trajectory length by going through "splits" representing trajectory segments forward and backward and returning the arithmetic mean of the calculated fractal dimension values. After the last step, the length of the remaining trajectory is estimated. The formula for calculating fractal dimension is given in Equation~\ref{eqn:3}, where $\varepsilon$ is the step size, $N(\varepsilon)$ the number of trajectory segments, and $D_{0}$ the fractal dimension.
  
\begin{equation}
D_{0}=\lim_{\varepsilon \rightarrow 0} \frac{\log{N(\varepsilon)}}{\log{\frac{1}{\varepsilon}}} \label{eqn:3}
\end{equation}
  
The range of step sizes used to calculate the fractal dimension must be predefined. The function \textit{TrajLogSequence} returns a specified number of points in a given range that can be used as step sizes, and the points are uniformly spaced if viewed on a logarithmic axis. A thousand values uniformly spaced between one and two kilometers on the logarithmic axis were used to estimate the fractal dimension of the aircraft's trajectory. An empirical assessment of the minimum and maximum aircraft speed and distance traveled in one ten-second step determines the lower and upper range boundaries.

A test using the function \textit{TrajFractalDimensionValues} returns the trajectory lengths calculated for a range of step sizes. If the relationship between step size and trajectory length is linear, then the trajectory is a fractal curve for a given range of step sizes. The direction in which the trajectory length is linearly dependent on the step size was estimated by linear regression using the \textit{lm} function from the \textit{stats} library.

\subsubsection{Direction change}
%\label{subsubsec:DC_SDDC}
 
Direction Change is the change in angle (in degrees) between two steps in the trajectory, divided by the time difference between the two steps. Kitamura and Imafuku \cite{Kitamura2015} originally used it to assess locomotor mimicry in butterflies. The direction changes between all adjacent step pairs are calculated using the function \textit{TrajDirectionalChange}. The arithmetic mean of direction change represents the non-linearity index and the standard deviation measures irregularity.

\subsubsection{Data preprocessing}
%\label{subsubsec:Preprocessing}

Before the data was used as input to classification models, each feature, or each column of the matrix, was scaled. When scaling, the arithmetic mean of all feature values (matrix column) is subtracted from the feature value for a given sample (trajectory), and the result is then divided by the standard deviation of the feature value (matrix column).

\subsubsection{Training and testing dataset}
%\label{subsubsec:DataTrainTest}

All trajectories in which the third point was east of the latitude of $16.0688$ degrees $\mathrm{E}$ or north of the longitude of $45.7429$ degrees $\mathrm{N}$ representing the Zagreb Pleso airport are marked as class $1$. Other trajectories that satisfied none of the two specified conditions are marked as class $-1$. Trajectories were smoothed and resampled to a constant time interval of $10$ $s$ between points before labeling. The third point was used because the first and second points were too close to the airport to distinguish between classes based on their coordinates.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 0.49 \linewidth]{all_2D.pdf}
    \caption{Classification of all trajectories based on the third step and location relative to the Zagreb Pleso airport.}
    \label{fig:labeling}
\end{figure}

The original set of observations has been split into training and testing subsets, considering the results of exploratory statistical, and outlier analysis, while sustaining proportions of data relating to classes of trajectories. $294$ original trajectories are used for training and testing. The samples are divided into training and testing datasets as close as possible to a ratio of $70\%$ for training and $30\%$ for testing. The division was stratified so that an approximately equal ratio of classes was present in both the training and testing data. Table~\ref{tab:classranges} lists the total number of samples in each class, and the number of samples used for testing, and training.

\begin{table}[!ht]
    \centering
    \caption{The total number of samples in each class, and the number of samples used for testing, and training. The total number of samples in any class used for training and testing is also listed.}
    \label{tab:classranges}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Class & Total samples & Test samples & Train samples \\ \hline
        1 & 200 & 71 & 129 \\ \hline
        -1 & 94 & 22 & 72 \\ \hline
        All & 294 & 93 & 201 \\ \hline
    \end{tabular}
\end{table}

It is evident from Table~\ref{tab:classranges} that class $1$ is around three times as common as class $-1$, impacting model performance.

\subsubsection{Distribution analysis for predictors}
%\label{subsubsec:Distribution}

Table~\ref{tab:avgsd} and Table~\ref{tab:minmax} provide the minimum, $1^{st}$ quartile, median, arithmetic mean, standard deviation, $3^{rd}$ quartile, and maximum values for all variables, suggesting that variables are not normally distributed.

\begin{table}[!ht]
    \centering
    \caption{The median, arithmetic mean, and standard deviation values for all variables.}
    \label{tab:avgsd}
        \begin{tabular}{|c|c|c|c|c|c|c|c|}
            \hline
            Variable & Median & Mean & Sd \\ \hline
            Diffusion distance & $45244.58$ & $43033.61$ & $8765.07$ \\ \hline
            Length & $56627.78$ & $53136.65$ & $12343.34$ \\ \hline
            Duration & $410$ & $384.116$ & $74.672$ \\ \hline
            Speed & $137.31$ & $136.692$ & $14.076$ \\ \hline
            Acceleration & $2.809 \times 10^{-1}$ & $2.683 \times 10^{-1}$ & $1.17 \times 10^{-1}$ \\ \hline
            Straightness & $8.154 \times 10^{-1}$ & $8.202 \times 10^{-1}$ & $8.041 \times 10^{-2}$ \\ \hline
            Sinuosity & $2.663 \times 10^{-3}$ & $2.962 \times 10^{-3}$ & $1.894 \times 10^{-3}$ \\ \hline
            Maximum expected displacement & $206.472$ & $213945.9$ & $3584062$ \\ \hline
            Direction change (arithmetic average) & $1.524 \times 10^{-1}$ & $1.721 \times 10^{-1}$ & $1.007 \times 10^{-1}$ \\ \hline   
            Direction change (standard deviation) & $2.415 \times 10^{-1}$ & $2.772 \times 10^{-1}$ & $2.632 \times 10^{-1}$ \\ \hline   
            Fractal dimension & $1.001$ & $1.001$ & $2.518 \times 10^{-3}$ \\ \hline
            Temperature & $16$ & $16.236$ & $9.081$ \\ \hline
            Air pressure at sea level & $762$ & $761.999$ & $5.856$ \\ \hline
            Air pressure at the measuring station & $752.6$ & $752.514$ & $5.728$ \\ \hline
            Relative humidity & $60$ & $61.334$ & $18.14$ \\ \hline
            Wind speed & $2.09$ & $2.854$ & $1.998$ \\ \hline
            Dew point & $8$ & $7.974$ & $7.376$ \\ \hline
        \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The minimum, $1^{st}$ quartile, $3^{rd}$ quartile, and maximum values for all variables.}
    \label{tab:minmax}
        \begin{tabular}{|c|c|c|c|c|c|c|c|}
            \hline
            Variable & Min. & $1^{st}$ Qu. & $3^{rd}$ Qu. & Max. \\ \hline
            Diffusion distance & $22785.48$ & $34392.51$ & $50887.14$ & $54349.69$ \\ \hline
            Length & $22785.89$ & $40090.68$ & $63180.26$ & $82608.51$ \\ \hline
            Duration & $120$ & $320$ & $440$ & $540$ \\ \hline
            Speed & $107.833$ & $128.075$ & $144.625$ & $212.095$ \\ \hline
            Acceleration & $-6.114 \times 10^{-1}$ & $2.392 \times 10^{-1}$ & $3.118 \times 10^{-1}$ & $9.134 \times 10^{-1}$ \\ \hline  
            Straightness & $5.92 \times 10^{-1}$ & $7.772 \times 10^{-1}$ & $8.643 \times 10^{-1}$ & $10.0 \times 10^{-1}$ \\ \hline     
            Sinuosity & $4.145 \times 10^{-6}$ & $2.433 \times 10^{-3}$ & $3.013 \times 10^{-3}$ & $1.666 \times 10^{-2}$ \\ \hline      
            Maximum expected & \multirow{2}{*}{$4.596$} &  \multirow{2}{*}{$172.644$} &  \multirow{2}{*}{$247.63$} &  \multirow{2}{*}{$61450080$} \\
            displacement & & & & \\ \hline
            Direction change & \multirow{2}{*}{$3.391 \times 10^{-4}$} & \multirow{2}{*}{$1.342 \times 10^{-1}$} & \multirow{2}{*}{$1.795 \times 10^{-1}$} & \multirow{2}{*}{$8.763 \times 10^{-1}$} \\
            (arithmetic average) & & & & \\ \hline
            Direction change & \multirow{2}{*}{$4.091 \times 10^{-4}$} & \multirow{2}{*}{$2.162 \times 10^{-1}$} & \multirow{2}{*}{$2.613 \times 10^{-1}$} & \multirow{2}{*}{$2.216$} \\
             (standard deviation) & & & & \\ \hline
            Fractal dimension & $1$ & $1.001$ & $1.002$ & $1.041$ \\ \hline
            Temperature & $-1$ & $9$ & $24$ & $35$ \\ \hline
            Air pressure at sea level & $746.3$ & $758.2$ & $765$ & $783.8$ \\ \hline
            Air pressure at the & \multirow{2}{*}{$736.8$} & \multirow{2}{*}{$749.128$} & \multirow{2}{*}{$755.65$} & \multirow{2}{*}{$773.6$} \\
            measuring station & & & & \\ \hline
            Relative humidity & $19$ & $45.962$ & $76$ & $100$ \\ \hline
            Wind speed & $0$ & $1.672$ & $4$ & $12$ \\ \hline
            Dew point & $-12$ & $2$ & $14$ & $21$ \\ \hline
        \end{tabular}
\end{table}

The Shapiro-Wilk normality tests, using the \textit{R} function \textit{shapiro.test}, did not yield a $p$-value larger than the selected $\alpha$-value of $0.05$ for any variable, further strengthening the claim based on Table~\ref{tab:minmax} that variables do not follow a normal (Gaussian) distribution.

\subsubsection{Correlation analysis for predictors}
%\label{subsubsec:Correlation}

Observations of statistical variables were assessed for their mutual association/correlation to identify the classification model structure, potential predictors, and targets. Figure~\ref{fig:correlation} contains a heat map of the correlation between all variables used in the study.

\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.99\linewidth]{corrplot.pdf}
    \caption{A heat map of the correlation between all variables used in this study. Red represents a high positive correlation, blue represents a high negative correlation, and white represents a low correlation. Variables are fully correlated with themselves, so values on the secondary diagonal equal $1$. The matrix is symmetrical concerning the secondary diagonal because the same combination of correlated variables is achieved when swapping the row and column indices.}
    \label{fig:correlation}
\end{figure}

The largest correlation coefficient in Figure~\ref{fig:correlation} depicts a significant correlation between sinuosity and the arithmetic average and standard deviation of direction change and equals $0.96$. This is due to the similarity in the calculation methods for direction change and sinuosity. The arithmetic average and standard deviation of direction change have a correlation coefficient of $0.91$ since they represent different characteristics of the same sequence of direction change values.

The length variable exhibits the second and third largest correlation coefficient in Figure~\ref{fig:correlation} when paired with duration or diffusion distance, equaling $0.93$ and $0.91$ respectively. Diffusion distance and duration have a correlation coefficient of $0.82$. The association between these variables seems logical since trajectory length increases with duration, and diffusion distance is usually larger for longer trajectories.

The speed variable demonstrates a correlation coefficient of $0.56$ and $0.51$ when paired with diffusion distance and trajectory length respectively. This occurs because planes gradually accelerate as they travel further from the airport and start cruising at larger altitudes.

Straightness has a negative correlation coefficient of $-0.55$ and $-0.59$ with trajectory length and duration respectively. There is a higher chance of a larger number of turns when the trajectory length and duration are larger, which decreases straightness.

The fractal dimension of a trajectory is strongly positively correlated with the maximum expected displacement, and the arithmetic average and standard deviation of direction change, with a correlation coefficient of $0.54$, $0.63$, and $0.6$ respectively.
 
Temperature and dew point are the meteorological features most strongly correlated with one another, exhibiting a correlation coefficient of $0.84$. This is expected because dew point depends on temperature as defined by the laws of physics. Similarly, the temperature and relative humidity have a strong negative correlation coefficient of $-0.53$.

Trajectory class is most strongly associated with diffusion distance, trajectory length, duration, and speed, with correlation coefficients of $0.78$, $0.72$, $0.6$, and $0.59$ respectively. This confirms that the assigned class can be predicted using trajectory features.

The box plots of all variables in Figure~\ref{fig:iono3boxplot} are used to support the correlations shown in Figure~\ref{fig:correlation} by exhibiting the trend of each variable.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{boxplot.pdf}
    \caption{Box plots of all variables used as predictors for trajectory class.}
    \label{fig:iono3boxplot}
\end{figure}

Figure~\ref{fig:iono3boxplot} shows the minimum, maximum, and arithmetic mean of the diffusion distance, trajectory length, duration, and speed increasing for trajectories in class $1$. The maximum expected displacement, relative humidity, air pressure at the measuring station, and sea level exhibit this trend, but it is less prominent, as they have a lower positive correlation in Figure~\ref{fig:correlation}. The opposite behavior is observed for all other variables, as indicated by a negative correlation in Figure~\ref{fig:correlation}.

\subsection{Confusion matrix}
%\label{subsec:Metrics}

The default approach to a confusion matrix uses only two groups (Yes and No, positive and negative). A correct classification is marked as a True Positive (TP), or True Negative (TN) result. A type I or II error in classification is marked as False Positive (FP) or False Negative (FN), respectively.

The percentage of correctly classified samples that truly belong to a class is evaluated by sensitivity, recall, hit rate, or True Positive Rate (TPR), calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FN})$, and specificity, selectivity, or True Negative Rate (TNR), calculated as $\mathrm{TN}/(\mathrm{TN}+\mathrm{FP})$ \cite{altman1994diagnostic1, altman1994diagnostic2}.

Precision, or Positive Predictive Value (PPV), calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP})$, and Negative Predictive Value (NPV), calculated as $\mathrm{TN}/(\mathrm{TN}+\mathrm{FN})$, represent the ratio of correct assignment among samples classified to a class.

The proportion of samples of a class or classifications to a class is recorded by Prevalence (P), calculated as $(\mathrm{TP}+\mathrm{FN})/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$, and Detection Prevalence (DP), calculated as $(\mathrm{TP}+\mathrm{FP})/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$, respectively. If prevalence is not equally distributed among classes, they are unbalanced.

The proportion of correct assignment for a single or any class is noted by the Detection Rate (DR), calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$, and Accuracy (Acc), calculated as $(\mathrm{TP} + \mathrm{TN}) / (\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN})$, respectively.

Balanced Accuracy (BA) \cite{velez2007balanced} is used for unbalanced classes since accuracy does not consider them individually. In this study, only $31.97\%$ of trajectories belong to the negative class. The formula $(\mathrm{TPR} + \mathrm{TNR}) / 2$ equals the arithmetic mean of TPR and TNR, focused on the positive and the negative classes, respectively.

Another metric that can be used instead of accuracy to consider each class separately is the $F1$ score, or the harmonic mean of PPV and TPR, calculated as $2 \times (\mathrm{PPV} \times \mathrm{TPR}) / (\mathrm{PPV} + \mathrm{TPR}) = 2 \times \mathrm{TP} / (2 \times \mathrm{TP} + \mathrm{FP} + \mathrm{FN})$.

\subsubsection{McNemar's test}
%\label{subsubsec:McNemar}

McNemar’s test is applied to paired classification results to test correlated proportions with the chi-squared distribution. It was conceived for binary classification. The null hypothesis of marginal homogeneity means marginal probabilities for all outcomes are the same. This can better describe model differences than a simple sensitivity and specificity comparison.

If a value smaller than $6$ occurs in the contingency table, the chi-squared distribution might not be appropriate. Edwards \cite{edwards1948note} proposed continuity correction using an approximation of the exact $p$-value for the binomial distribution, as shown in Equation~\ref{eqn:4}. In Equation~\ref{eqn:4}, $b$ is the number of samples the first method labeled as positive, while the second assigned a contradicting label. Similarly, $c$ is the number of samples the second method labeled as positive, while the first assigned a contradicting label.
 
\begin{equation}
    \chi^{2}=\frac{(|b-c|-1)^{2}}{b+c}
    \label{eqn:4}
\end{equation}

The Bonferroni correction \cite{bonferroni1936teoria} can account for the increased likelihood of incorrectly rejecting a null hypothesis when conducting multiple tests, equal to making a Type I error \cite{mittelhammer2000econometric}. This procedure can also be used on confidence intervals, as proposed by Dunn \cite{dunn1961multiple}. The significance level $\alpha$ is divided by the number of tests \cite{rupert2012simultaneous}. A total of $20$ models are compared in the statistical tests in this experiment, using $10$ types of classifiers and $2$ sets of predictors. The sets of predictors encompass all trajectory features or only diffusion distance and the arithmetic average of direction change. The number of tests equals $20 * 19 / 2 = 10 * 19 = 190$. With a standard value of $\alpha = 0.05$, the Bonferroni correction tests each hypothesis at $0.05 / 190 = 2.63 \times 10 ^{-4}$.

\section{Research results}
\label{sec:Results}

Candidate models were assessed to determine the optimal method and set of predictors to be used in the final model for application in real settings. The performance metrics of the candidate models is listed in Table~\ref{tab:acc:all}, Table~\ref{tab:acc:noMETAR}, Table~\ref{tab:acc:METAR} and Table~\ref{tab:acc:dist_dc}. The execution time in seconds ($s$) utilizing the \textit{R} \textit{system.time} function for all candidate models is displayed in Table~\ref{tab:time}. Research results are presented with eleven trajectory-derived predictors: diffusion distance, trajectory length, duration, speed, acceleration, straightness, sinuosity, maximum expected displacement, the arithmetic average and standard deviation of direction change, and fractal dimension. The following six meteorological features were also included: temperature, air pressure at sea level and the measuring station, relative humidity, wind speed, and dew point. Candidate models have been developed using: (1.): trajectory and meteorological features (Table~\ref{tab:acc:all}) (2.): exclusively trajectory features (Table~\ref{tab:acc:noMETAR}) (3.): exclusively meteorological features (Table~\ref{tab:acc:METAR}), and (4.): exclusively diffusion distance and the arithmetic average of direction change (Table~\ref{tab:acc:dist_dc}). The reasoning behind this approach was the assumption that reducing the set of predictors would reduce model complexity and computation time. It is theoretically sound that the impact of changes in meteorological features is reflected in the movement of the aircraft and the trajectory features describing it, so adding additional data is redundant. Diffusion distance and the arithmetic average of direction change were chosen to establish a model with a minimal predictor set since they describe the static and dynamic aspects of the trajectory expressed as a scalar and vector variable, in that order. This pair of predictors was also used in the final model. The highest accuracy was achieved in Table~\ref{tab:acc:dist_dc} using diffusion distance and the arithmetic average of direction change as predictors with the Gaussian Process (GP) classifier method. This supports the hypothesis that meteorological features and trajectory features other than diffusion distance and the arithmetic average of direction change should be removed from the set of predictors. The Gaussian Process method yielded the model with the highest accuracy, leading to the decision that it should be used in the final model.

\begin{table}[!ht]
    \centering
    \caption{The performance metrics for each candidate model developed using different methods and trajectory and meteorological features as predictors. The bold values indicate the best value for each performance metric. All metrics except FP, FN, FNR, FPR, FDR, and FOR are maximized. The values N and P in brackets represent the Negative and Positive classes respectively.}
	\label{tab:acc:all}
    	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
    		\hline
            \multicolumn{11}{|c|}{Trajectory and Meteorological Features as Predictors} \\ \hline
             & \multicolumn{10}{|c|}{Classifier Method} \\ \hline
            \multirow{9}{*}{\rotatebox{90}{Metric}} & & & & & & & & & & \\
             & \multirow{9}{*}{\rotatebox{90}{k-Nearest} \rotatebox{90}{Neighbours}} & \multirow{9}{*}{\rotatebox{90}{Linear Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Radial Basis} \rotatebox{90}{Function Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Gaussian} \rotatebox{90}{Process}} & \multirow{9}{*}{\rotatebox{90}{Decision} \rotatebox{90}{Tree}} & \multirow{9}{*}{\rotatebox{90}{Random} \rotatebox{90}{Forest}} & \multirow{9}{*}{\rotatebox{90}{Naive} \rotatebox{90}{Bayes}} & \multirow{9}{*}{\rotatebox{90}{Multilayer} \rotatebox{90}{Perceptron}} & \multirow{9}{*}{\rotatebox{90}{AdaBoost}} & \multirow{9}{*}{\rotatebox{90}{Quadratic} \rotatebox{90}{Discriminant Analysis}} \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\
             & & & & & & & & & & \\ \hline
            TP & $\mathbf{65}$ & $62$ & $\mathbf{65}$ & $64$ & $62$ & $64$ & $\mathbf{65}$ & $62$ & $63$ & $64$ \\ \hline
            TN & $20$ & $\mathbf{22}$ & $\mathbf{22}$ & $\mathbf{22}$ & $21$ & $\mathbf{22}$ & $21$ & $\mathbf{22}$ & $\mathbf{22}$ & $\mathbf{22}$ \\ \hline
            FP & $2$ & $\mathbf{0}$ & $\mathbf{0}$ & $\mathbf{0}$ & $1$ & $\mathbf{0}$ & $1$ & $\mathbf{0}$ & $\mathbf{0}$ & $\mathbf{0}$ \\ \hline
            FN & $\mathbf{6}$ & $9$ & $\mathbf{6}$ & $7$ & $9$ & $7$ & $\mathbf{6}$ & $9$ & $8$ & $7$ \\ \hline
            TPR & $\mathbf{91.55}$ & $87.32$ & $\mathbf{91.55}$ & $90.14$ & $87.32$ & $90.14$ & $\mathbf{91.55}$ & $87.32$ & $88.73$ & $90.14$ \\ \hline
            FNR & $\mathbf{8.45}$ & $12.68$ & $\mathbf{8.45}$ & $9.86$ & $12.68$ & $9.86$ & $\mathbf{8.45}$ & $12.68$ & $11.27$ & $9.86$ \\ \hline
            TNR & $90.91$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $95.45$ & $\mathbf{100.0}$ & $95.45$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ \\ \hline
            FPR & $9.09$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $4.55$ & $\mathbf{0.0}$ & $4.55$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ \\ \hline
            PPV & $97.01$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $98.41$ & $\mathbf{100.0}$ & $98.48$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ \\ \hline
            FDR & $2.99$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $1.59$ & $\mathbf{0.0}$ & $1.52$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ \\ \hline
            NPV & $76.92$ & $70.97$ & $\mathbf{78.57}$ & $75.86$ & $70.0$ & $75.86$ & $77.78$ & $70.97$ & $73.33$ & $75.86$ \\ \hline
            FOR & $23.08$ & $29.03$ & $\mathbf{21.43}$ & $24.14$ & $30.0$ & $24.14$ & $22.22$ & $29.03$ & $26.67$ & $24.14$ \\ \hline
            DR & \multirow{2}{*}{$\mathbf{69.89}$} & \multirow{2}{*}{$66.67$} & \multirow{2}{*}{$\mathbf{69.89}$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$66.67$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$\mathbf{69.89}$} & \multirow{2}{*}{$66.67$} & \multirow{2}{*}{$67.74$} & \multirow{2}{*}{$68.82$} \\
            (P) & & & & & & & & & & \\ \hline
            DR & \multirow{2}{*}{$21.51$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} \\
            (N) & & & & & & & & & & \\ \hline
            DP & \multirow{2}{*}{$\mathbf{72.04}$} & \multirow{2}{*}{$66.67$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$67.74$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$66.67$} & \multirow{2}{*}{$67.74$} & \multirow{2}{*}{$68.82$} \\
            (P) & & & & & & & & & & \\ \hline
            DP & \multirow{2}{*}{$27.96$} & \multirow{2}{*}{$\mathbf{33.33}$} & \multirow{2}{*}{$30.11$} & \multirow{2}{*}{$31.18$} & \multirow{2}{*}{$32.26$} & \multirow{2}{*}{$31.18$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$\mathbf{33.33}$} & \multirow{2}{*}{$32.26$} & \multirow{2}{*}{$31.18$} \\     
            (N) & & & & & & & & & & \\ \hline
            Acc & $91.4$ & $90.32$ & $\mathbf{93.55}$ & $92.47$ & $89.25$ & $92.47$ & $92.47$ & $90.32$ & $91.4$ & $92.47$ \\ \hline
            BA & $91.23$ & $93.66$ & $\mathbf{95.77}$ & $95.07$ & $91.39$ & $95.07$ & $93.5$ & $93.66$ & $94.37$ & $95.07$ \\ \hline
            F1 & $94.2$ & $93.23$ & $\mathbf{95.59}$ & $94.81$ & $92.54$ & $94.81$ & $94.89$ & $93.23$ & $94.03$ & $94.81$ \\ \hline
    	\end{tabular}
\end{table}

With trajectory and meteorological features used as predictors, the Radial Basis Function (RBF) Support Vector Machine (SVM) is the most successful classifier in Table~\ref{tab:acc:all}. It has the most TP and TN classifications, and the highest TPR, TNR, PPV, NPV, DR for both classes, accuracy, BA, and F1 score values. The RBF SVM method has the least FN and FP classifications, and the lowest FNR, FPR, FDR, and FOR. The Linear and RBF SVM, GP, Random Forest (RF), Multilayer Perceptron (MLP), AdaBoost (AB), and Quadratic Discriminant Analysis (QDA) methods make no misclassifications for the negative class, as indicated by the highest number of TN classifications, no FP classifications, the highest DR for the negative class, a TNR and PPV of $100\%$, and FPR and FDR equal to $0\%$. Concerning the positive class, the k-Nearest Neighbours (k-NN) and Naive Bayes (NB) methods are tied with the RBF SVM method in terms of TP and FN classifications, TPR, FNR, and DR for the positive class. The k-NN method has the highest DP for the positive class, while the Linear SVM and MLP methods have the highest DP for the negative class.
        
\begin{table}[!ht]
    \centering
    \caption{The performance metrics for each candidate model developed using different methods and trajectory features as predictors. The bold values indicate the best value for each performance metric. All metrics except FP, FN, FNR, FPR, FDR, and FOR are maximized. The values N and P in brackets represent the Negative and Positive classes respectively.}
	\label{tab:acc:noMETAR}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
        \multicolumn{11}{|c|}{Trajectory Features as Predictors} \\ \hline
         & \multicolumn{10}{|c|}{Classifier Method} \\ \hline
        \multirow{9}{*}{\rotatebox{90}{Metric}} & & & & & & & & & & \\
         & \multirow{9}{*}{\rotatebox{90}{k-Nearest} \rotatebox{90}{Neighbours}} & \multirow{9}{*}{\rotatebox{90}{Linear Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Radial Basis} \rotatebox{90}{Function Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Gaussian} \rotatebox{90}{Process}} & \multirow{9}{*}{\rotatebox{90}{Decision} \rotatebox{90}{Tree}} & \multirow{9}{*}{\rotatebox{90}{Random} \rotatebox{90}{Forest}} & \multirow{9}{*}{\rotatebox{90}{Naive} \rotatebox{90}{Bayes}} & \multirow{9}{*}{\rotatebox{90}{Multilayer} \rotatebox{90}{Perceptron}} & \multirow{9}{*}{\rotatebox{90}{AdaBoost}} & \multirow{9}{*}{\rotatebox{90}{Quadratic} \rotatebox{90}{Discriminant Analysis}} \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\ \hline
        TP & $65$ & $66$ & $66$ & $66$ & $60$ & $65$ & $65$ & $63$ & $\mathbf{70}$ & $64$ \\ \hline
        TN & $21$ & $\mathbf{22}$ & $\mathbf{22}$ & $\mathbf{22}$ & $21$ & $\mathbf{22}$ & $21$ & $21$ & $21$ & $\mathbf{22}$ \\ \hline
        FP & $1$ & $\mathbf{0}$ & $\mathbf{0}$ & $\mathbf{0}$ & $1$ & $\mathbf{0}$ & $1$ & $1$ & $1$ & $\mathbf{0}$ \\ \hline
        FN & $6$ & $5$ & $5$ & $5$ & $11$ & $6$ & $6$ & $8$ & $\mathbf{1}$ & $7$ \\ \hline
        TPR & $91.55$ & $92.96$ & $92.96$ & $92.96$ & $84.51$ & $91.55$ & $91.55$ & $88.73$ & $\mathbf{98.59}$ & $90.14$ \\ \hline
        FNR & $8.45$ & $7.04$ & $7.04$ & $7.04$ & $15.49$ & $8.45$ & $8.45$ & $11.27$ & $\mathbf{1.41}$ & $9.86$ \\ \hline
        TNR & $95.45$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $95.45$ & $\mathbf{100.0}$ & $95.45$ & $95.45$ & $95.45$ & $\mathbf{100.0}$ \\ \hline
        FPR & $4.55$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $4.55$ & $\mathbf{0.0}$ & $4.55$ & $4.55$ & $4.55$ & $\mathbf{0.0}$ \\ \hline
        PPV & $98.48$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $\mathbf{100.0}$ & $98.36$ & $\mathbf{100.0}$ & $98.48$ & $98.44$ & $98.59$ & $\mathbf{100.0}$ \\ \hline
        FDR & $1.52$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $\mathbf{0.0}$ & $1.64$ & $\mathbf{0.0}$ & $1.52$ & $1.56$ & $1.41$ & $\mathbf{0.0}$ \\ \hline
        NPV & $77.78$ & $81.48$ & $81.48$ & $81.48$ & $65.62$ & $78.57$ & $77.78$ & $72.41$ & $\mathbf{95.45}$ & $75.86$ \\ \hline
        FOR & $22.22$ & $18.52$ & $18.52$ & $18.52$ & $34.38$ & $21.43$ & $22.22$ & $27.59$ & $\mathbf{4.55}$ & $24.14$ \\ \hline
        DR & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$64.52$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$67.74$} & \multirow{2}{*}{$\mathbf{75.27}$} & \multirow{2}{*}{$68.82$} \\
        (P) & & & & & & & & & & \\ \hline
        DR & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} \\
        (N) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$65.59$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$\mathbf{76.34}$} & \multirow{2}{*}{$68.82$} \\
        (P) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$\mathbf{34.41}$} & \multirow{2}{*}{$30.11$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$31.18$} & \multirow{2}{*}{$23.66$} & \multirow{2}{*}{$31.18$} \\
        (N) & & & & & & & & & & \\ \hline
        Acc & $92.47$ & $94.62$ & $94.62$ & $94.62$ & $87.1$ & $93.55$ & $92.47$ & $90.32$ & $\mathbf{97.85}$ & $92.47$ \\ \hline
        BA & $93.5$ & $96.48$ & $96.48$ & $96.48$ & $89.98$ & $95.77$ & $93.5$ & $92.09$ & $\mathbf{97.02}$ & $95.07$ \\ \hline
        F1 & $94.89$ & $96.35$ & $96.35$ & $96.35$ & $90.91$ & $95.59$ & $94.89$ & $93.33$ & $\mathbf{98.59}$ & $94.81$ \\ \hline
	\end{tabular}
\end{table}
        
The AB method is the most successful classifier if trajectory features are provided as the only model input, as presented in Table~\ref{tab:acc:METAR}. The AB method has the most TP and least FN classifications. It also has the highest TPR, NPV, positive class DR and DP, accuracy, BA, and F1 score values, and the lowest FNR and FOR values. The Linear and RBF SVM, GP, RF, and QDA methods classify all negative samples correctly since they exhibit the highest number of TN classifications, no FP classifications, the highest DR for the negative class, a TNR and PPV of $100\%$, and FPR and FDR equal to $0\%$. The Decision Tree (DT) method has the highest DP for the negative class.

\begin{table}[!ht]
    \centering
    \caption{The performance metrics for each candidate model developed using different methods and meteorological features as predictors. The bold values indicate the best value for each performance metric. All metrics except FP, FN, FNR, FPR, FDR, and FOR are maximized. The values N and P in brackets represent the Negative and Positive classes respectively.}
	\label{tab:acc:METAR}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
        \multicolumn{11}{|c|}{Meteorological Features as Predictors} \\ \hline
         & \multicolumn{10}{|c|}{Classifier Method} \\ \hline
        \multirow{9}{*}{\rotatebox{90}{Metric}} & & & & & & & & & & \\
         & \multirow{9}{*}{\rotatebox{90}{k-Nearest} \rotatebox{90}{Neighbours}} & \multirow{9}{*}{\rotatebox{90}{Linear Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Radial Basis} \rotatebox{90}{Function Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Gaussian} \rotatebox{90}{Process}} & \multirow{9}{*}{\rotatebox{90}{Decision} \rotatebox{90}{Tree}} & \multirow{9}{*}{\rotatebox{90}{Random} \rotatebox{90}{Forest}} & \multirow{9}{*}{\rotatebox{90}{Naive} \rotatebox{90}{Bayes}} & \multirow{9}{*}{\rotatebox{90}{Multilayer} \rotatebox{90}{Perceptron}} & \multirow{9}{*}{\rotatebox{90}{AdaBoost}} & \multirow{9}{*}{\rotatebox{90}{Quadratic} \rotatebox{90}{Discriminant Analysis}} \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\ \hline
        TP & $57$ & $65$ & $\mathbf{66}$ & $64$ & $51$ & $53$ & $50$ & $57$ & $52$ & $60$ \\ \hline
        TN & $5$ & $3$ & $4$ & $5$ & $\mathbf{9}$ & $8$ & $\mathbf{9}$ & $6$ & $7$ & $4$ \\ \hline
        FP & $17$ & $19$ & $18$ & $17$ & $\mathbf{13}$ & $14$ & $\mathbf{13}$ & $16$ & $15$ & $18$ \\ \hline
        FN & $14$ & $6$ & $\mathbf{5}$ & $7$ & $20$ & $18$ & $21$ & $14$ & $19$ & $11$ \\ \hline
        TPR & $80.28$ & $91.55$ & $\mathbf{92.96}$ & $90.14$ & $71.83$ & $74.65$ & $70.42$ & $80.28$ & $73.24$ & $84.51$ \\ \hline
        FNR & $19.72$ & $8.45$ & $\mathbf{7.04}$ & $9.86$ & $28.17$ & $25.35$ & $29.58$ & $19.72$ & $26.76$ & $15.49$ \\ \hline
        TNR & $22.73$ & $13.64$ & $18.18$ & $22.73$ & $\mathbf{40.91}$ & $36.36$ & $\mathbf{40.91}$ & $27.27$ & $31.82$ & $18.18$ \\ \hline
        FPR & $77.27$ & $86.36$ & $81.82$ & $77.27$ & $\mathbf{59.09}$ & $63.64$ & $\mathbf{59.09}$ & $72.73$ & $68.18$ & $81.82$ \\ \hline
        PPV & $77.03$ & $77.38$ & $78.57$ & $79.01$ & $\mathbf{79.69}$ & $79.1$ & $79.37$ & $78.08$ & $77.61$ & $76.92$ \\ \hline
        FDR & $22.97$ & $22.62$ & $21.43$ & $20.99$ & $\mathbf{20.31}$ & $20.9$ & $20.63$ & $21.92$ & $22.39$ & $23.08$ \\ \hline
        NPV & $26.32$ & $33.33$ & $\mathbf{44.44}$ & $41.67$ & $31.03$ & $30.77$ & $30.0$ & $30.0$ & $26.92$ & $26.67$ \\ \hline
        FOR & $73.68$ & $66.67$ & $\mathbf{55.56}$ & $58.33$ & $68.97$ & $69.23$ & $70.0$ & $70.0$ & $73.08$ & $73.33$ \\ \hline
        DR & \multirow{2}{*}{$61.29$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$\mathbf{70.97}$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$54.84$} & \multirow{2}{*}{$56.99$} & \multirow{2}{*}{$53.76$} & \multirow{2}{*}{$61.29$} & \multirow{2}{*}{$55.91$} & \multirow{2}{*}{$64.52$} \\
        (P) & & & & & & & & & & \\ \hline
        DR & \multirow{2}{*}{$5.38$} & \multirow{2}{*}{$3.23$} & \multirow{2}{*}{$4.3$} & \multirow{2}{*}{$5.38$} & \multirow{2}{*}{$\mathbf{9.68}$} & \multirow{2}{*}{$8.6$} & \multirow{2}{*}{$\mathbf{9.68}$} & \multirow{2}{*}{$6.45$} & \multirow{2}{*}{$7.53$} & \multirow{2}{*}{$4.3$} \\
        (N) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$79.57$} & \multirow{2}{*}{$\mathbf{90.32}$} & \multirow{2}{*}{$\mathbf{90.32}$} & \multirow{2}{*}{$87.1$} & \multirow{2}{*}{$68.82$} & \multirow{2}{*}{$72.04$} & \multirow{2}{*}{$67.74$} & \multirow{2}{*}{$78.49$} & \multirow{2}{*}{$72.04$} & \multirow{2}{*}{$83.87$} \\      
        (P) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$20.43$} & \multirow{2}{*}{$9.68$} & \multirow{2}{*}{$9.68$} & \multirow{2}{*}{$12.9$} & \multirow{2}{*}{$31.18$} & \multirow{2}{*}{$27.96$} & \multirow{2}{*}{$\mathbf{32.26}$} & \multirow{2}{*}{$21.51$} & \multirow{2}{*}{$27.96$} & \multirow{2}{*}{$16.13$} \\
        (N) & & & & & & & & & & \\ \hline
        Acc & $66.67$ & $73.12$ & $\mathbf{75.27}$ & $74.19$ & $64.52$ & $65.59$ & $63.44$ & $67.74$ & $63.44$ & $68.82$ \\ \hline
        BA & $51.5$ & $52.59$ & $55.57$ & $\mathbf{56.43}$ & $56.37$ & $55.51$ & $55.67$ & $53.78$ & $52.53$ & $51.34$ \\ \hline
        F1 & $78.62$ & $83.87$ & $\mathbf{85.16}$ & $84.21$ & $75.56$ & $76.81$ & $74.63$ & $79.17$ & $75.36$ & $80.54$ \\ \hline
	\end{tabular}
\end{table}

Using exclusively meteorological predictors results in the RBF SVM method being the most accurate, with the highest number of TP classifications, TPR, NPV, positive class DR and DP, accuracy, and F1 score values, listed in Table~\ref{tab:acc:METAR}. The RBF SVM method has the lowest number of FN classifications, FNR, and FOR. The Linear SVM method has the highest positive class DP, the same as the RBF SVM method. The DT and NB methods achieve the best performance for the negative class, as demonstrated by the highest number of TN classifications, no FP classifications, the highest TNR and negative class DR, and the lowest FPR. Additionally, the DT method has the highest PPV and lowest FDR value, and the GP method has the highest BA value. The NB method has the highest negative class DP.
        
\begin{table}[!ht]
    \centering
    \caption{The performance metrics for each candidate model developed using different methods and diffusion distance and the arithmetic average of direction change as predictors. The bold values indicate the best value for each performance metric. All metrics except FP, FN, FNR, FPR, FDR, and FOR are maximized. The values N and P in brackets represent the Negative and Positive classes respectively.}
	\label{tab:acc:dist_dc}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
        \multicolumn{11}{|c|}{Diffusion Distance and the Arithmetic Average of Direction Change as Predictors} \\ \hline
         & \multicolumn{10}{|c|}{Classifier Method} \\ \hline
        \multirow{9}{*}{\rotatebox{90}{Metric}} & & & & & & & & & & \\
         & \multirow{9}{*}{\rotatebox{90}{k-Nearest} \rotatebox{90}{Neighbours}} & \multirow{9}{*}{\rotatebox{90}{Linear Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Radial Basis} \rotatebox{90}{Function Support} \rotatebox{90}{Vector Machine}} & \multirow{9}{*}{\rotatebox{90}{Gaussian} \rotatebox{90}{Process}} & \multirow{9}{*}{\rotatebox{90}{Decision} \rotatebox{90}{Tree}} & \multirow{9}{*}{\rotatebox{90}{Random} \rotatebox{90}{Forest}} & \multirow{9}{*}{\rotatebox{90}{Naive} \rotatebox{90}{Bayes}} & \multirow{9}{*}{\rotatebox{90}{Multilayer} \rotatebox{90}{Perceptron}} & \multirow{9}{*}{\rotatebox{90}{AdaBoost}} & \multirow{9}{*}{\rotatebox{90}{Quadratic} \rotatebox{90}{Discriminant Analysis}} \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\
         & & & & & & & & & & \\ \hline
        TP & $68$ & $60$ & $66$ & $\mathbf{70}$ & $65$ & $67$ & $61$ & $68$ & $66$ & $62$ \\ \hline
        TN & $21$ & $21$ & $21$ & $21$ & $21$ & $21$ & $\mathbf{22}$ & $21$ & $20$ & $\mathbf{22}$ \\ \hline
        FP & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ & $\mathbf{0}$ & $1$ & $2$ & $\mathbf{0}$ \\ \hline
        FN & $3$ & $11$ & $5$ & $\mathbf{1}$ & $6$ & $4$ & $10$ & $3$ & $5$ & $9$ \\ \hline
        TPR & $95.77$ & $84.51$ & $92.96$ & $\mathbf{98.59}$ & $91.55$ & $94.37$ & $85.92$ & $95.77$ & $92.96$ & $87.32$ \\ \hline
        FNR & $4.23$ & $15.49$ & $7.04$ & $\mathbf{1.41}$ & $8.45$ & $5.63$ & $14.08$ & $4.23$ & $7.04$ & $12.68$ \\ \hline
        TNR & $95.45$ & $95.45$ & $95.45$ & $95.45$ & $95.45$ & $95.45$ & $\mathbf{100.0}$ & $95.45$ & $90.91$ & $\mathbf{100.0}$ \\ \hline
        FPR & $4.55$ & $4.55$ & $4.55$ & $4.55$ & $4.55$ & $4.55$ & $\mathbf{0.0}$ & $4.55$ & $9.09$ & $\mathbf{0.0}$ \\ \hline
        PPV & $98.55$ & $98.36$ & $98.51$ & $98.59$ & $98.48$ & $98.53$ & $\mathbf{100.0}$ & $98.55$ & $97.06$ & $\mathbf{100.0}$ \\ \hline
        FDR & $1.45$ & $1.64$ & $1.49$ & $1.41$ & $1.52$ & $1.47$ & $\mathbf{0.0}$ & $1.45$ & $2.94$ & $\mathbf{0.0}$ \\ \hline
        NPV & $87.5$ & $65.62$ & $80.77$ & $\mathbf{95.45}$ & $77.78$ & $84.0$ & $68.75$ & $87.5$ & $80.0$ & $70.97$ \\ \hline
        FOR & $12.5$ & $34.38$ & $19.23$ & $\mathbf{4.55}$ & $22.22$ & $16.0$ & $31.25$ & $12.5$ & $20.0$ & $29.03$ \\ \hline
        DR & \multirow{2}{*}{$73.12$} & \multirow{2}{*}{$64.52$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$\mathbf{75.27}$} & \multirow{2}{*}{$69.89$} & \multirow{2}{*}{$72.04$} & \multirow{2}{*}{$65.59$} & \multirow{2}{*}{$73.12$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$66.67$} \\
        (P) & & & & & & & & & & \\ \hline
        DR & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$\mathbf{23.66}$} & \multirow{2}{*}{$22.58$} & \multirow{2}{*}{$21.51$} & \multirow{2}{*}{$\mathbf{23.66}$} \\     
        (N) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$74.19$} & \multirow{2}{*}{$65.59$} & \multirow{2}{*}{$72.04$} & \multirow{2}{*}{$\mathbf{76.34}$} & \multirow{2}{*}{$70.97$} & \multirow{2}{*}{$73.12$} & \multirow{2}{*}{$65.59$} & \multirow{2}{*}{$74.19$} & \multirow{2}{*}{$73.12$} & \multirow{2}{*}{$66.67$} \\
        (P) & & & & & & & & & & \\ \hline
        DP & \multirow{2}{*}{$25.81$} & \multirow{2}{*}{$\mathbf{34.41}$} & \multirow{2}{*}{$27.96$} & \multirow{2}{*}{$23.66$} & \multirow{2}{*}{$29.03$} & \multirow{2}{*}{$26.88$} & \multirow{2}{*}{$\mathbf{34.41}$} & \multirow{2}{*}{$25.81$} & \multirow{2}{*}{$26.88$} & \multirow{2}{*}{$33.33$} \\     
        (N) & & & & & & & & & & \\ \hline
        Acc & $95.7$ & $87.1$ & $93.55$ & $\mathbf{97.85}$ & $92.47$ & $94.62$ & $89.25$ & $95.7$ & $92.47$ & $90.32$ \\ \hline
        BA & $95.61$ & $89.98$ & $94.21$ & $\mathbf{97.02}$ & $93.5$ & $94.91$ & $92.96$ & $95.61$ & $91.93$ & $93.66$ \\ \hline
        F1 & $97.14$ & $90.91$ & $95.65$ & $\mathbf{98.59}$ & $94.89$ & $96.4$ & $92.42$ & $97.14$ & $94.96$ & $93.23$ \\ \hline
	\end{tabular}
\end{table}

The diffusion distance and the arithmetic average of direction change as predictors yield the GP method as the best overall performer in Table~\ref{tab:acc:dist_dc}, with the highest number of TP classifications, TPR, NPV, positive class DR and DP, accuracy, BA, and F1 score values. The GP method has the lowest number of FN classifications, FNR, and FOR. The NB and QDA methods achieve perfect results for the negative class, as indicated by the highest number of TN classifications, highest DR for the negative class, no FP classifications, a TNR and PPV of $100\%$, and FPR and FDR equal to $0\%$. The NB and Linear SVM methods have the highest DP for the negative class.

The study was conducted on the \textit{Windows} 11 operating system with \textit{R Studio} version 2024.04.2+764 and \textit{R} version 4.4.1, the AMD Radeon RX 6600 Graphics Processing Unit (GPU), $16$ GB of Random Access Memory (RAM), running the AMD Ryzen 5 PRO 4650G Central Processing Unit (CPU) with $6$ cores. Execution time is releveant because built-in systems for mobile devices and embedded systems have low computational capabilities. The candidate models using the k-NN method in Table~\ref{tab:time} have the highest execution time. The extensive hyperparameter running process explains this. AB is the only other classifier with execution time over one second, due to the iterative boosting approach. k-NN and AB were excluded from further consideration for the final model due to these challenges.

\begin{table}[!ht]
    \centering
    \caption{The execution time in seconds ($s$) for each candidate model developed using different methods and sets of predictors.}
	\label{tab:time}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
        & \multicolumn{4}{|c|}{Sets of Predictors} \\ \hline
         & Trajectory & & & Diffusion Distance \\
        Classifier & and & Trajectory & Meteorological & and the \\
        Method & Meteorological & Features & Features & Arithmetic Average \\
         & Features & & & of Direction Change \\ \hline
        k-Nearest Neighbours & $2.42$ & $2.26$ & $2.25$ & $2.06$ \\ \hline
        Linear Support & \multirow{2}{*}{$0.01$} & \multirow{2}{*}{$<0.01$} & \multirow{2}{*}{$<0.01$} & \multirow{2}{*}{$<0.01$} \\
        Vector Machine & & & & \\ \hline
        Radial Basis & & & & \\
        Function Support & $<0.01$ & $<0.01$ & $0.01$ & $<0.01$ \\
        Vector Machine & & & & \\ \hline
        Gaussian Process & $0.15$ & $0.12$ & $0.07$ & $0.1$ \\ \hline
        Decision Tree & $0.12$ & $0.14$ & $0.13$ & $0.14$ \\ \hline
        Random Forest & $0.07$ & $0.05$ & $0.08$ & $0.05$ \\ \hline
        Naive Bayes & $<0.01$ & $0.02$ & $0.02$ & $<0.01$ \\ \hline
        Multilayer Perceptron & $0.89$ & $0.72$ & $0.61$ & $0.47$ \\ \hline
        AdaBoost & $1.22$ & $1$ & $0.79$ & $0.63$ \\ \hline
        Quadratic & \multirow{2}{*}{$<0.01$} & \multirow{2}{*}{$0.01$} & \multirow{2}{*}{$0.01$} & \multirow{2}{*}{$<0.01$} \\
        Discriminant Analysis & & & & \\ \hline
	\end{tabular}
\end{table}

The performance assessment of candidate models revealed the success of several methods, such as the Gaussian Process (GP) and AdaBoost classifiers. Accuracy results in Table~\ref{tab:acc:all}, Table~\ref{tab:acc:noMETAR}, Table~\ref{tab:acc:METAR} and Table~\ref{tab:acc:dist_dc} may lead to an incorrect conclusion as the dataset is unbalanced, as shown in Table~\ref{tab:classranges}. Further assessment is needed to determine if a candidate model differs significantly from others. The results of McNemar’s test are presented only for candidate models using either exclusively trajectory features or diffusion distance and the arithmetic average of direction change as predictor variables. These sets of predictors yielded the most accurate among tested candidate models compared to those using both trajectory and meteorological features or exclusively meteorological features.

\subsection{Results of McNemar's test}
%\label{subsec:ResultsMcNemar}

McNemar’s test can help assess if a candidate model performs significantly worse or better than others. Figure~\ref{fig:mcnemarplot} contains $p$-values of McNemar's test when comparing candidate models using either exclusively trajectory features or diffusion distance and the arithmetic average of direction change as predictor variables and various classification methods. A lower $p$-value indicates that the classifications are unequal for a pair of candidate models. A higher $p$-value suggests that the classifications are equal for a pair of candidate models.

\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.99\linewidth]{mcnemarplot.pdf}
    \caption{Candidate model comparison using $p$-values of McNemar's test with various classification methods combined with trajectory features or diffusion distance and the arithmetic average of direction change as predictor variables. Black represents a low $p$-value near $0$, white represents a high $p$-value near $1$, and red represents a $p$-value near $0.5$ between the two extremes. Each candidate model is equal to itself, so values on the primary diagonal equal $1$. The matrix is symmetrical concerning the primary diagonal because the same result is achieved when swapping the order of the first and second compared candidate models.}
    \label{fig:mcnemarplot}
\end{figure}

The classifications are not statistically significantly different when comparing the candidate models using any tested classification method combined with trajectory features or diffusion distance and the arithmetic average of direction change as predictors, as shown in Figure~\ref{fig:mcnemarplot}. The $p$-values of McNemar's test larger than the significance level of $2.63 \times 10 ^{-4}$, which was adjusted from an alpha value of $0.05$ with the Bonferroni correction, indicate that any of the compared models might yield similar results and could be used to infer trajectory class. The diffusion distance and the arithmetic average of direction change as predictors combined with the Gaussian Process (GP) classifier method yield the best accuracy in Table~\ref{tab:acc:dist_dc}, so this was chosen as the final model.

\section{Discussion}
\label{sec:Discussion}

The k-Nearest Neighbours (k-NN), and AdaBoost (AB) methods have the highest execution time, above $1$ $s$, so they were excluded from further analysis and application in the final model.

The AB method has a high execution time for any set of predictors due to an extensive hyperparameter tuning procedure during training, evident from the data in Table~\ref{tab:time}. However, it achieved a $97.85\%$ accuracy using all trajectory features as predictors, shown in Table~\ref{tab:acc:noMETAR}. The Gaussian Process (GP) method has the same accuracy when using diffusion distance and the arithmetic average of direction change as predictors, presented in Table~\ref{tab:acc:dist_dc}, and the training time for the AB method is around ten times longer, so the GP method is preferred as the approach in the final model.

All candidate models developed using exclusively meteorological features fail to achieve an accuracy over $90\%$, ranging between $63\%$ and $75\%$. This indicates that these meteorological features are less suitable as predictors for this particular application. All models using at least two trajectory features achieved an accuracy larger than $87\%$, suggesting they could be applied in practice.

\section{Conclusion}
\label{sec:Conclusion}

The presented study aims to classify trajectories originating from a chosen origin airport and heading for a target destination airport. Classification models were applied to descriptions of the trajectories using multiple features and meteorological data. It was assumed observations contained independent variables to generate the dependent variable representing the trajectory class. The trajectories were labeled based on the first three points of the trajectory relative to the start airport coordinates. Statistical analysis confirmed that trajectory features change distribution based on trajectory class.

The k-Nearest Neighbours (k-NN), Linear and Radial Basis Function (RBF) Support Vector Machine (SVM), Gaussian Process (GP), Decision Tree (DT), Random Forest (RF), Naive Bayes (NB), Multilayer Perceptron (MLP), AdaBoost (AB), and Quadratic Discriminant Analysis (QDA) method created a trajectory classification from multiple combinations of input variables.

The GP method using the arithmetic average of direction change and diffusion distance as predictors achieved a $97.85\%$ testing accuracy. The total execution time is around ten times shorter than the AdaBoost method using all tested trajectory features, making it more suitable for compact, low-performance, and low-cost portable devices such as smartphones. The exclusion of meteorological features and other trajectory features besides the arithmetic average of direction change and diffusion distance from the final is supported by their theoretical definitions, representing both dynamic and static features of the trajectory using vector and scalar values.

\section{Declarations}
%\label{sec:Declarations}

\subsection{Availability of data and materials}
%\label{subsec:Availability}

The datasets used during the current study are available from the corresponding author upon reasonable request.

\subsection{Competing interests}
%\label{subsec:Competing}

The authors declare no conflict of interest.

\subsection{Funding}
%\label{subsec:Funding}

The authors have no funding sources to declare.

\subsection{Authors' contributions}
%\label{subsec:Contributions}

L\v{Z} contributed to conceptualization, methodology, software, validation, formal analysis, investigation, data curation, original draft writing, text review and editing, and visualization. RF contributed to conceptualization, methodology, investigation, resources, original draft writing, text review and editing, and supervision. All authors read and approved the final manuscript.

\subsection{Acknowledgements}
%\label{subsec:Acknowledgements}

Not applicable

\subsection{Authors' information}
%\label{subsec:Information}

L\v{Z} is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Science. Her research interests include applied machine learning in biology, chemistry, medicine, and transportation. Maritime transportation is also the subject of her doctoral research.

RF is an external Professor of Electronics Engineering with the Department for Computer Science, Faculty of Engineering, and Center for Artificial Intelligence and Cybersecurity, both at the University of Rijeka, Croatia, and Head of the Laboratory for Spatial Intelligence at Hrvatsko Zagorje Krapina University of Applied Science, Krapina, Croatia. He holds a BSc, MSc, and PhD in electrical engineering, obtained in 1987, 1994, and 2007, respectively, from the Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia. His professional interests include Ambient-Aware Application-Aligned ((AA)2) Global Navigation Satellite System (GNSS) Positioning, Navigation, and Timing (PNT), spatial uncertainty quantification, spatial statistical learning, predictive modeling, statistical signal processing, trajectory analysis and prediction, and occupancy modeling. Professor Filjar is a Fellow of The Royal Institute of Navigation (RIN) (London, United Kingdom), a Member of The Institute of Navigation (ION) (Manassas, Virginia), a Senior Member of Union Radio-Scientifique Internationale (URSI) (Ghent, Belgium) and a Member of The Society for Industrial and Applied Mathematics  (Philadelphia, Philadelphia).

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

%\bibliography{sn-bibliography}% common bib file
%%\input sn-article.bbl
\begin{thebibliography}{101}
% BibTex style file: bmc-mathphys.bst (version 2.1), 2014-07-24
\ifx \bisbn   \undefined \def \bisbn  #1{ISBN #1}\fi
\ifx \binits  \undefined \def \binits#1{#1}\fi
\ifx \bauthor  \undefined \def \bauthor#1{#1}\fi
\ifx \batitle  \undefined \def \batitle#1{#1}\fi
\ifx \bjtitle  \undefined \def \bjtitle#1{\textit{#1}}\fi
\ifx \bvolume  \undefined \def \bvolume#1{\textit{#1}}\fi
\ifx \byear  \undefined \def \byear#1{#1}\fi
\ifx \bissue  \undefined \def \bissue#1{#1}\fi
\ifx \bfpage  \undefined \def \bfpage#1{#1}\fi
\ifx \blpage  \undefined \def \blpage #1{#1}\fi
\ifx \burl  \undefined \def \burl#1{\textsf{#1}}\fi
\ifx \doiurl  \undefined \def \doiurl#1{\url{https://doi.org/#1}}\fi
\ifx \betal  \undefined \def \betal{\textit{et al.}}\fi
\ifx \binstitute  \undefined \def \binstitute#1{#1}\fi
\ifx \binstitutionaled  \undefined \def \binstitutionaled#1{#1}\fi
\ifx \bctitle  \undefined \def \bctitle#1{#1}\fi
\ifx \beditor  \undefined \def \beditor#1{#1}\fi
\ifx \bpublisher  \undefined \def \bpublisher#1{#1}\fi
\ifx \bbtitle  \undefined \def \bbtitle#1{\textit{#1}}\fi
\ifx \bedition  \undefined \def \bedition#1{#1}\fi
\ifx \bseriesno  \undefined \def \bseriesno#1{#1}\fi
\ifx \blocation  \undefined \def \blocation#1{#1}\fi
\ifx \bsertitle  \undefined \def \bsertitle#1{#1}\fi
\ifx \bsnm \undefined \def \bsnm#1{#1}\fi
\ifx \bsuffix \undefined \def \bsuffix#1{#1}\fi
\ifx \bparticle \undefined \def \bparticle#1{#1}\fi
\ifx \barticle \undefined \def \barticle#1{#1}\fi
\bibcommenthead
\ifx \bconfdate \undefined \def \bconfdate #1{#1}\fi
\ifx \botherref \undefined \def \botherref #1{#1}\fi
\ifx \url \undefined \def \url#1{\textsf{#1}}\fi
\ifx \bchapter \undefined \def \bchapter#1{#1}\fi
\ifx \bbook \undefined \def \bbook#1{#1}\fi
\ifx \bcomment \undefined \def \bcomment#1{#1}\fi
\ifx \oauthor \undefined \def \oauthor#1{#1}\fi
\ifx \citeauthoryear \undefined \def \citeauthoryear#1{#1}\fi
\ifx \endbibitem  \undefined \def \endbibitem {}\fi
\ifx \bconflocation  \undefined \def \bconflocation#1{#1}\fi
\ifx \arxivurl  \undefined \def \arxivurl#1{\textsf{#1}}\fi
\csname PreBibitemsHook\endcsname

%%% 1
\bibitem{McLean2018-jr}
\begin{barticle}
\bauthor{\bsnm{McLean}, \binits{D.J.}}, \&
\bauthor{\bsnm{Skowron Volponi}, \binits{M.}}
(\byear{2018}).
\batitle{trajr: {A}n {R} package for characterisation of animal trajectories}.
\bjtitle{Ethology},
\bvolume{124},
\bfpage{440}--\bfpage{448}.
doi:10.1111/eth.12739.
\end{barticle}
\endbibitem

%%% 2
\bibitem{dumenvcic2022trajectory}
\begin{barticle}
\bauthor{\bsnm{Dumen{\v{c}}i{\'c}}, \binits{S.}},
\bauthor{\bsnm{{\v{Z}}u{\v{z}}i{\'c}}, \binits{L.}}, \&
\bauthor{\bsnm{Bre{\v{s}}}, \binits{B.}}
(\byear{2022}).
\batitle{{T}rajectory {A}nalysis of {G}riffon {V}ulture in {K}varner {B}ay}.
\bjtitle{The Journal of CIEES},
\bvolume{2},
\bfpage{36}--\bfpage{42}.
doi:10.48149/jciees.2022.2.2.6.
\end{barticle}
\endbibitem

%%% 3
\bibitem{Makar2022}
\begin{botherref}
\oauthor{\bsnm{Makar}, \binits{D.}}
(\byear{2022}).
{A}naliza putanja zrakoplova kori{\v{s}}tenjem opa{\v{z}}anja iz mre{\v{z}}e {O}pen{S}ky {N}etwork.
\url{https://urn.nsk.hr/urn:nbn:hr:190:547850}.
Accessed 30 November 2024.
\end{botherref}
\endbibitem

%%% 4
\bibitem{filjarairplane}
\begin{bchapter}
\bauthor{\bsnm{Filjar}, \binits{R.}}, \&
\bauthor{\bsnm{Spudi{\'c}}, \binits{R.}}
(\byear{2023}).
\bctitle{{A}irplane {T}rajectory {R}econstruction and {A}nalysis {U}sing {G}{N}{S}{S}-{B}ased {A}{D}{S}-{B} {D}ata: {W}hat to {D}o with the {O}pen {A}ccess {D}ata {U}sing {O}pen-{S}ource {S}oftware}.
In \bbtitle{{U}nited {N}ations/{F}inland {W}orkshop on the {A}pplications of {G}lobal {N}avigation {S}atellite {S}ystems}.
\end{bchapter}
\endbibitem

%%% 5
\bibitem{Gultepe2019}
\begin{barticle}
\bauthor{\bsnm{Gultepe}, \binits{I.}},
\bauthor{\bsnm{Sharman}, \binits{R.}},
\bauthor{\bsnm{Williams}, \binits{P.D.}},
\bauthor{\bsnm{Zhou}, \binits{B.}},
\bauthor{\bsnm{Ellrod}, \binits{G.P.}},
\bauthor{\bsnm{Minnis}, \binits{P.}},
\bauthor{\bsnm{Trier}, \binits{S.}},
\bauthor{\bsnm{Griffin}, \binits{S.}},
\bauthor{\bsnm{Yum}, \binits{S.S.}},
\bauthor{\bsnm{Gharabaghi}, \binits{B.}},
\bauthor{\bsnm{Feltz}, \binits{W.}},
\bauthor{\bsnm{Temimi}, \binits{M.}},
\bauthor{\bsnm{Dimri}, \binits{A.P.}},
\bauthor{\bsnm{Dietz}, \binits{S.}},
\bauthor{\bsnm{Fran{\c{c}}a}, \binits{G.B.}},
\bauthor{\bsnm{Almeida}, \binits{M.V.}},
\bauthor{\bsnm{Albquerque}, \binits{F.}},
\bauthor{\bsnm{Pu}, \binits{Z.}},
\bauthor{\bsnm{Kneringer}, \binits{P.}},
\bauthor{\bsnm{Weston}, \binits{M.}},
\bauthor{\bsnm{Chuang}, \binits{H.Y.}}, \&
\bauthor{\bsnm{Thobois}, \binits{L.}}
(\byear{2019}).
\batitle{{A} {R}eview of {H}igh {I}mpact {W}eather for {A}viation {M}eteorology}.
\bjtitle{Pure and Applied Geophysics},
\bvolume{176},
\bfpage{1869}--\bfpage{1921}.
doi:10.1007/s00024-019-02168-6.
\end{barticle}
\endbibitem

%%% 6
\bibitem{csl-202004-0019}
\begin{barticle}
\bauthor{\bsnm{Filjar}, \binits{R.}},
\bauthor{\bsnm{Sklebar}, \binits{I.}}, \&
\bauthor{\bsnm{Horvat}, \binits{M.}}
(\byear{2020}).
\batitle{{A} {C}omparison of {M}achine {L}earning-{B}ased {I}ndividual {M}obility {C}lassification {M}odels {D}eveloped on {S}ensor {R}eadings from {L}oosely {A}ttached {S}martphones}.
\bjtitle{{C}ommunications - {S}cientific {L}etters of the {U}niversity of {Z}ilina},
\bvolume{22},
\bfpage{153}--\bfpage{162}.
doi:10.26552/com.C.2020.4.153-162.
\end{barticle}
\endbibitem

%%% 7
\bibitem{Garcia_Ceja2021-tw}
\begin{bbook}
\bauthor{\bsnm{Garcia Ceja}, \binits{E.}}
(\byear{2021}).
\bbtitle{{B}ehavior analysis with machine learning using {R}}.
\blocation{Philadelphia, PA}: 
\bpublisher{Chapman \& Hall/CRC}.
\end{bbook}
\endbibitem

%%% 8
\bibitem{Bosson2018}
\begin{bchapter}
\bauthor{\bsnm{Bosson}, \binits{C.}}, \&
\bauthor{\bsnm{Nikoleris}, \binits{T.}}
(\byear{2018}).
\bctitle{{S}upervised {L}earning {A}pplied to {A}ir {T}raffic {T}rajectory {C}lassification}.
In \bbtitle{2018 {A}{I}{A}{A} {I}nformation {S}ystems-{A}{I}{A}{A} {I}nfotech}
(pp. \bfpage{}).
doi:10.2514/6.2018-1637.
\end{bchapter}
\endbibitem

%%% 9
\bibitem{Kegl2013}
\begin{barticle}
\bauthor{\bsnm{K{\'e}gl}, \binits{B.}}
(\byear{2013}).
\batitle{{T}he return of {A}da{B}oost.{M}{H}: multi-class {H}amming trees}.
\bjtitle{International Conference on Learning Representations}.
doi:10.48550/arXiv.1312.6086.
\end{barticle}
\endbibitem

%%% 10
\bibitem{Fix1989}
\begin{barticle}
\bauthor{\bsnm{Fix}, \binits{E.}}, \&
\bauthor{\bsnm{Hodges}, \binits{J.L.}}
(\byear{1989}).
\batitle{{D}iscriminatory {A}nalysis. {N}onparametric {D}iscrimination: {C}onsistency {P}roperties}.
\bjtitle{International Statistical Review},
\bvolume{57},
\bfpage{238}--\bfpage{247}.
doi:10.2307/1403797.
\end{barticle}
\endbibitem

%%% 11
\bibitem{Cover1967}
\begin{barticle}
\bauthor{\bsnm{Cover}, \binits{T.M.}}, \&
\bauthor{\bsnm{Hart}, \binits{P.E.}}
(\byear{1967}).
\batitle{{N}earest neighbor pattern classification}.
\bjtitle{IEEE Trans. Inf. Theory},
\bvolume{13},
\bfpage{21}--\bfpage{27}.
doi:10.1109/TIT.1967.1053964.
\end{barticle}
\endbibitem

%%% 12
\bibitem{Hastie2009}
\begin{barticle}
\bauthor{\bsnm{Hastie}, \binits{T.}},
\bauthor{\bsnm{Rosset}, \binits{S.}},
\bauthor{\bsnm{Zhu}, \binits{J.}}, \&
\bauthor{\bsnm{Zou}, \binits{H.}}
(\byear{2009}).
\batitle{{M}ulti-class {A}da{B}oost}.
\bjtitle{Stat. Interface},
\bvolume{2},
\bfpage{349}--\bfpage{360}.
doi:10.4310/SII.2009.v2.n3.a8.
\end{barticle}
\endbibitem

%%% 13
\bibitem{Everitt2011}
\begin{bbook}
\bauthor{\bsnm{Everitt}, \binits{B.S.}},
\bauthor{\bsnm{Landau}, \binits{S.}},
\bauthor{\bsnm{Leese}, \binits{M.}}, \&
\bauthor{\bsnm{Stahl}, \binits{D.}}
(\byear{2011}).
\bbtitle{{C}luster {A}nalysis}.
\blocation{Hoboken, New Jersey, United States of America}: 
\bpublisher{Wiley}.
\end{bbook}
\endbibitem

%%% 14
\bibitem{Hall2008}
\begin{barticle}
\bauthor{\bsnm{Hall}, \binits{P.}},
\bauthor{\bsnm{Park}, \binits{B.U.}}, \&
\bauthor{\bsnm{Samworth}, \binits{R.J.}}
(\byear{2008}).
\batitle{{C}hoice of neighbor order in nearest-neighbor classification}.
\bjtitle{Ann. Stat.},
\bvolume{36},
\bfpage{2135}--\bfpage{2152}.
doi:10.1214/07-AOS537.
\end{barticle}
\endbibitem

%%% 15
\bibitem{Kuhn2007}
\begin{barticle}
\bauthor{\bsnm{Kuhn}, \binits{M.}}
(\byear{2007}).
\batitle{caret: {C}lassification and {R}egression {T}raining}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/cran.package.caret.
\end{barticle}
\endbibitem

%%% 16
\bibitem{Rasmussen2005}
\begin{bbook}
\bauthor{\bsnm{Rasmussen}, \binits{C.E.}}, \&
\bauthor{\bsnm{Williams}, \binits{C.K.I.}}
(\byear{2005}).
\bbtitle{{G}aussian processes for machine learning}.
\blocation{Cambridge, Massachusetts}: 
\bpublisher{MIT Press}.
\end{bbook}
\endbibitem

%%% 17
\bibitem{Liu2010}
\begin{bbook}
\bauthor{\bsnm{Liu}, \binits{W.}},
\bauthor{\bsnm{Principe}, \binits{J.C.}}, \&
\bauthor{\bsnm{Haykin}, \binits{S.S.}}
(\byear{2010}).
\bbtitle{{K}ernel adaptive filtering}.
\blocation{Hoboken, NJ}: 
\bpublisher{Wiley-Blackwell}.
\end{bbook}
\endbibitem

%%% 18
\bibitem{Karatzoglou2004}
\begin{barticle}
\bauthor{\bsnm{Karatzoglou}, \binits{A.}},
\bauthor{\bsnm{Smola}, \binits{A.}}, \&
\bauthor{\bsnm{Hornik}, \binits{K.}}
(\byear{2004}).
\batitle{kernlab: {K}ernel-{B}ased {M}achine {L}earning {L}ab}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.kernlab.
\end{barticle}
\endbibitem

%%% 19
\bibitem{Williams1998}
\begin{barticle}
\bauthor{\bsnm{Williams}, \binits{C.K.I.}}, \&
\bauthor{\bsnm{Barber}, \binits{D.}}
(\byear{1998}).
\batitle{{B}ayesian classification with {G}aussian processes}.
\bjtitle{IEEE Transactions on Pattern Analysis and Machine Intelligence},
\bvolume{20},
\bfpage{1342}--\bfpage{1351}.
doi:10.1109/34.735807.
\end{barticle}
\endbibitem

%%% 20
\bibitem{Ho1995}
\begin{bchapter}
\bauthor{\bsnm{Ho}, \binits{T.K.}}
(\byear{1995}).
\bctitle{{R}andom decision forests}.
In \bbtitle{{P}roceedings of 3rd {I}nternational {C}onference on {D}ocument {A}nalysis and {R}ecognition}
(pp. \bfpage{278}--\bfpage{282}).
doi:10.1109/ICDAR.1995.598994.
\end{bchapter}
\endbibitem

%%% 21
\bibitem{Kleinberg1990}
\begin{barticle}
\bauthor{\bsnm{Kleinberg}, \binits{E.M.}}
(\byear{1990}).
\batitle{{S}tochastic discrimination}.
\bjtitle{Annals of Mathematics and Artificial Intelligence},
\bvolume{1},
\bfpage{207}--\bfpage{239}.
doi:10.1007/BF01531079.
\end{barticle}
\endbibitem

%%% 22
\bibitem{Kleinberg1996}
\begin{barticle}
\bauthor{\bsnm{Kleinberg}, \binits{E.M.}}
(\byear{1996}).
\batitle{{A}n overtraining-resistant stochastic modeling method for pattern recognition}.
\bjtitle{Ann. Stat.},
\bvolume{24},
\bfpage{2319}--\bfpage{2349}.
doi:10.1214/aos/1032181157.
\end{barticle}
\endbibitem

%%% 23
\bibitem{Kleinberg2000}
\begin{barticle}
\bauthor{\bsnm{Kleinberg}, \binits{E.M.}}
(\byear{2000}).
\batitle{{O}n the algorithmic implementation of stochastic discrimination}.
\bjtitle{IEEE Trans. Pattern Anal. Mach. Intell.},
\bvolume{22},
\bfpage{473}--\bfpage{490}.
doi:10.1109/34.857004.
\end{barticle}
\endbibitem

%%% 24
\bibitem{Breiman2001}
\begin{barticle}
\bauthor{\bsnm{Breiman}, \binits{L.}}
(\byear{2001}).
\batitle{{R}andom {F}orests}.
\bjtitle{Machine Learning},
\bvolume{45},
\bfpage{5}--\bfpage{32}.
doi:10.1023/A:1010933404324.
\end{barticle}
\endbibitem

%%% 25
\bibitem{Breiman2002}
\begin{barticle}
\bauthor{\bsnm{Breiman}, \binits{L.}},
\bauthor{\bsnm{Cutler}, \binits{A.}},
\bauthor{\bsnm{Liaw}, \binits{A.}}, \&
\bauthor{\bsnm{Wiener}, \binits{M.}}
(\byear{2002}).
\batitle{random{F}orest: {B}reiman and {C}utlers {R}andom {F}orests for {C}lassification and {R}egression}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.randomForest.
\end{barticle}
\endbibitem

%%% 26
\bibitem{Amit1997}
\begin{barticle}
\bauthor{\bsnm{Amit}, \binits{Y.}}, \&
\bauthor{\bsnm{Geman}, \binits{D.}}
(\byear{1997}).
\batitle{{S}hape quantization and recognition with randomized trees}.
\bjtitle{Neural Comput.},
\bvolume{9},
\bfpage{1545}--\bfpage{1588}.
doi:10.1162/neco.1997.9.7.1545.
\end{barticle}
\endbibitem

%%% 27
\bibitem{Cybenko1989}
\begin{barticle}
\bauthor{\bsnm{Cybenko}, \binits{G.}}
(\byear{1989}).
\batitle{{A}pproximation by superpositions of a sigmoidal function}.
\bjtitle{Mathematics of Control, Signals and Systems},
\bvolume{2},
\bfpage{303}--\bfpage{314}.
doi:10.1007/BF02551274.
\end{barticle}
\endbibitem

%%% 28
\bibitem{Linnainmaa1976}
\begin{barticle}
\bauthor{\bsnm{Linnainmaa}, \binits{S.}}
(\byear{1976}).
\batitle{{T}aylor expansion of the accumulated rounding error}.
\bjtitle{BIT Numerical Mathematics},
\bvolume{16},
\bfpage{146}--\bfpage{160}.
doi:10.1007/BF01931367.
\end{barticle}
\endbibitem

%%% 29
\bibitem{Kelley1960}
\begin{barticle}
\bauthor{\bsnm{Kelley}, \binits{H.J.}}
(\byear{1960}).
\batitle{{G}radient theory of optimal flight paths}.
\bjtitle{ARS J.},
\bvolume{30},
\bfpage{947}--\bfpage{954}.
doi:10.2514/8.5282.
\end{barticle}
\endbibitem

%%% 30
\bibitem{Rosenblatt1962}
\begin{bbook}
\bauthor{\bsnm{Rosenblatt}, \binits{F.}}
(\byear{1962}).
\bbtitle{{P}rinciples of {N}eurodynamics: {P}erceptrons and the {T}heory of {B}rain {M}echanisms}.
\blocation{Lymington, Hampshire, United Kingdom}: 
\bpublisher{Spartan books}.
\end{bbook}
\endbibitem

%%% 31
\bibitem{Werbos1970}
\begin{barticle}
\bauthor{\bsnm{Werbos}, \binits{P.}}
(\byear{1970}).
\batitle{{A}pplications of advances in nonlinear sensitivity analysis}.
\bjtitle{System Modeling and Optimization},
\bvolume{38},
\bfpage{762}--\bfpage{770}.
doi:10.1007/BFb0006203.
\end{barticle}
\endbibitem

%%% 32
\bibitem{Rumelhart1987}
\begin{bchapter}
\bauthor{\bsnm{Rumelhart}, \binits{D.E.}}, \&
\bauthor{\bsnm{McClelland}, \binits{J.L.}}
(\byear{1987}).
\bctitle{{L}earning {I}nternal {R}epresentations by {E}rror {P}ropagation}.
In \bbtitle{{P}arallel {D}istributed {P}rocessing: {E}xplorations in the {M}icrostructure of {C}ognition: {F}oundations}
(pp. \bfpage{318}--\bfpage{362}).
\blocation{Cambridge, Massachusetts}: 
\bpublisher{MIT Press}.
\end{bchapter}
\endbibitem

%%% 33
\bibitem{Blansche2019}
\begin{barticle}
\bauthor{\bsnm{Blansch{\'e}}, \binits{A.}}
(\byear{2019}).
\batitle{fdm2id: {D}ata {M}ining and {R} {P}rogramming for {B}eginners}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.fdm2id.
\end{barticle}
\endbibitem

%%% 34
\bibitem{Ripley2009}
\begin{barticle}
\bauthor{\bsnm{Ripley}, \binits{B.}}
(\byear{2009}).
\batitle{nnet: {F}eed-{F}orward {N}eural {N}etworks and {M}ultinomial {L}og-{L}inear {M}odels}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.nnet.
\end{barticle}
\endbibitem

%%% 35
\bibitem{Boser1992}
\begin{bchapter}
\bauthor{\bsnm{Boser}, \binits{Bernhard E.}},
\bauthor{\bsnm{Guyon}, \binits{Isabelle M.}}, \&
\bauthor{\bsnm{Vapnik}, \binits{Vladimir N.}}
(\byear{1992}).
\bctitle{{A} training algorithm for optimal margin classifiers}.
In \bbtitle{{P}roceedings of the fifth annual workshop on {C}omputational learning theory}.
\blocation{New York, NY, USA}: 
\bpublisher{ACM}.
\end{bchapter}
\endbibitem

%%% 36
\bibitem{HastieRosset2009}
\begin{barticle}
\bauthor{\bsnm{Hastie}, \binits{T.}},
\bauthor{\bsnm{Rosset}, \binits{S.}},
\bauthor{\bsnm{Zhu}, \binits{J.}}, \&
\bauthor{\bsnm{Zou}, \binits{H.}}
(\byear{2009}).
\batitle{{M}ulti-class {A}da{B}oost}.
\bjtitle{Stat. Interface},
\bvolume{2},
\bfpage{349}--\bfpage{360}.
doi:10.4310/SII.2009.v2.n3.a8.
\end{barticle}
\endbibitem

%%% 37
\bibitem{Meyer2003}
\begin{barticle}
\bauthor{\bsnm{Meyer}, \binits{D.}},
\bauthor{\bsnm{Leisch}, \binits{F.}}, \&
\bauthor{\bsnm{Hornik}, \binits{K.}}
(\byear{2003}).
\batitle{{T}he support vector machine under test}.
\bjtitle{Neurocomputing},
\bvolume{55},
\bfpage{169}--\bfpage{186}.
doi:10.1016/S0925-2312(03)00431-4.
\end{barticle}
\endbibitem

%%% 38
\bibitem{Press2007}
\begin{bbook}
\bauthor{\bsnm{Press}, \binits{William H.}},
\bauthor{\bsnm{Teukolsky}, \binits{Saul A.}},
\bauthor{\bsnm{Vetterling}, \binits{William T.}}, \&
\bauthor{\bsnm{Flannery}, \binits{Brian P.}}
(\byear{2007}).
\bbtitle{{N}umerical recipes 3rd edition}.
\blocation{Cambridge, England}: 
\bpublisher{Cambridge University Press}.
\end{bbook}
\endbibitem

%%% 39
\bibitem{Meyer1999}
\begin{barticle}
\bauthor{\bsnm{Meyer}, \binits{D.}},
\bauthor{\bsnm{Dimitriadou}, \binits{E.}},
\bauthor{\bsnm{Hornik}, \binits{K.}},
\bauthor{\bsnm{Weingessel}, \binits{A.}}, \&
\bauthor{\bsnm{Leisch}, \binits{F.}}
(\byear{1999}).
\batitle{e1071: {M}isc {F}unctions of the {D}epartment of {S}tatistics, {P}robability {T}heory {G}roup ({F}ormerly: {E}1071)}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.e1071.
\end{barticle}
\endbibitem

%%% 40
\bibitem{Chang2007}
\begin{barticle}
\bauthor{\bsnm{Chang}, \binits{C.C.}}, \&
\bauthor{\bsnm{Lin}, \binits{C.J.}}
(\byear{2011}).
\batitle{{L}{I}{B}{S}{V}{M}: {A} library for support vector machines}.
\bjtitle{ACM Trans. Intell. Syst. Technol.},
\bvolume{2}.
doi:10.1145/1961189.1961199.
\end{barticle}
\endbibitem

%%% 41
\bibitem{Fan2008}
\begin{barticle}
\bauthor{\bsnm{Fan}, \binits{R.E.}},
\bauthor{\bsnm{Chang}, \binits{K.W.}},
\bauthor{\bsnm{Hsieh}, \binits{C.J.}},
\bauthor{\bsnm{Wang}, \binits{X.R.}}, \&
\bauthor{\bsnm{Lin}, \binits{C.J.}}
(\byear{2008}).
\batitle{{L}{I}{B}{L}{I}{N}{E}{A}{R}: a library for large linear classification}.
\bjtitle{Journal of Machine Learning Research},
\bvolume{9},
\bfpage{1871}--\bfpage{1874}.
doi:10.1145/1390681.1442794.
\end{barticle}
\endbibitem

%%% 42
\bibitem{Wu2008}
\begin{barticle}
\bauthor{\bsnm{Wu}, \binits{X.}},
\bauthor{\bsnm{Kumar}, \binits{V.}},
\bauthor{\bsnm{Quinlan}, \binits{R.J.}},
\bauthor{\bsnm{Ghosh}, \binits{J.}},
\bauthor{\bsnm{Yang}, \binits{Q.}},
\bauthor{\bsnm{Motoda}, \binits{H.}},
\bauthor{\bsnm{McLachlan}, \binits{G.J.}},
\bauthor{\bsnm{Ng}, \binits{A.}},
\bauthor{\bsnm{Liu}, \binits{B.}},
\bauthor{\bsnm{Yu}, \binits{P.S.}},
\bauthor{\bsnm{Zhou}, \binits{Z.H.}},
\bauthor{\bsnm{Steinbach}, \binits{M.}},
\bauthor{\bsnm{Hand}, \binits{D.J.}}, \&
\bauthor{\bsnm{Steinberg}, \binits{D.}}
(\byear{2008}).
\batitle{{T}op 10 algorithms in data mining}.
\bjtitle{Knowl. Inf. Syst.},
\bvolume{14},
\bfpage{1}--\bfpage{37}.
doi:10.1007/s10115-007-0114-2.
\end{barticle}
\endbibitem

%%% 43
\bibitem{ShalevShwartz2014}
\begin{bchapter}
\bauthor{\bsnm{Shalev-Shwartz}, \binits{S.}}, \&
\bauthor{\bsnm{Ben-David}, \binits{S.}}
(\byear{2014}).
\bctitle{{D}ecision {T}rees}.
In \bbtitle{{U}nderstanding {M}achine {L}earning: {F}rom {T}heory to {A}lgorithms}
(pp. \bfpage{212}--\bfpage{218}).
\blocation{Cambridge, United Kingdom}: 
\bpublisher{Cambridge University Press}.
doi:10.1017/CBO9781107298019.019.
\end{bchapter}
\endbibitem

%%% 44
\bibitem{Therneau1999}
\begin{barticle}
\bauthor{\bsnm{Therneau}, \binits{T.}}, \&
\bauthor{\bsnm{Atkinson}, \binits{B.}}
(\byear{1999}).
\batitle{rpart: {R}ecursive {P}artitioning and {R}egression {T}rees}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.rpart.
\end{barticle}
\endbibitem

%%% 45
\bibitem{Breiman1984}
\begin{bbook}
\bauthor{\bsnm{Breiman}, \binits{L.}},
\bauthor{\bsnm{Friedman}, \binits{J.}},
\bauthor{\bsnm{Stone}, \binits{C.J.}}, \&
\bauthor{\bsnm{Olshen}, \binits{R.A.}}
(\byear{1984}).
\bbtitle{{C}lassification and {R}egression {T}rees}.
\blocation{Milton Park, Abingdon-on-Thames, Oxfordshire, United Kingdom}: 
\bpublisher{Taylor \& Francis}.
\end{bbook}
\endbibitem

%%% 46
\bibitem{Hand2001}
\begin{barticle}
\bauthor{\bsnm{Hand}, \binits{D.J.}}, \&
\bauthor{\bsnm{Yu}, \binits{K.}}
(\byear{2001}).
\batitle{{I}diot's {B}ayes: {N}ot {S}o {S}tupid after {A}ll?}.
\bjtitle{Int. Stat. Rev.},
\bvolume{69},
\bfpage{385}.
doi:10.1111/j.1751-5823.2001.tb00465.x.
\end{barticle}
\endbibitem

%%% 47
\bibitem{Russell1999}
\begin{bbook}
\bauthor{\bsnm{Russell}, \binits{S.}}, \&
\bauthor{\bsnm{Norvig}, \binits{P.}}
(\byear{1999}).
\bbtitle{{A}rtificial intelligence}.
\blocation{Upper Saddle River, NJ}: 
\bpublisher{Pearson}.
\end{bbook}
\endbibitem

%%% 48
\bibitem{Caruana2006}
\begin{barticle}
\bauthor{\bsnm{Caruana}, \binits{R.}}, \&
\bauthor{\bsnm{Niculescu-Mizil}, \binits{A.}}
(\byear{2006}).
\batitle{{A}n {E}mpirical {C}omparison of {S}upervised {L}earning {A}lgorithms}.
\bjtitle{Proceedings of the 23rd international conference on Machine learning - ICML '06},
\bvolume{2006},
\bfpage{161}--\bfpage{168}.
doi:10.1145/1143844.1143865.
\end{barticle}
\endbibitem

%%% 49
\bibitem{John2013}
\begin{barticle}
\bauthor{\bsnm{John}, \binits{G.H.}}, \&
\bauthor{\bsnm{Langley}, \binits{P.}}
(\byear{2013}).
\batitle{{E}stimating continuous distributions in {B}ayesian classifiers}.
\bjtitle{Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)}.
doi:10.48550/arXiv.1302.4964.
\end{barticle}
\endbibitem

%%% 50
\bibitem{Murty2011}
\begin{bbook}
\bauthor{\bsnm{Murty}, \binits{M.N.}}, \&
\bauthor{\bsnm{Devi}, \binits{V.S.}}
(\byear{2011}).
\bbtitle{{P}attern {R}ecognition: {A}n {A}lgorithmic {A}pproach}.
\blocation{London, England}: 
\bpublisher{Springer}.
\end{bbook}
\endbibitem

%%% 51
\bibitem{Majka2017}
\begin{barticle}
\bauthor{\bsnm{Majka}, \binits{M.}}
(\byear{2017}).
\batitle{naivebayes: {H}igh {P}erformance {I}mplementation of the {N}aive {B}ayes {A}lgorithm}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.naivebayes.
\end{barticle}
\endbibitem

%%% 52
\bibitem{Olson2016}
\begin{barticle}
\bauthor{\bsnm{Olson}, \binits{M.}}
(\byear{2016}).
\batitle{{J}{O}{U}{S}{B}oost: {I}mplements {U}nder/{O}versampling for {P}robability {E}stimation}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.JOUSBoost.
\end{barticle}
\endbibitem

%%% 53
\bibitem{Freund1997}
\begin{barticle}
\bauthor{\bsnm{Freund}, \binits{Y.}}, \&
\bauthor{\bsnm{Schapire}, \binits{R.E.}}
(\byear{1997}).
\batitle{{A} {D}ecision-{T}heoretic {G}eneralization of {O}n-{L}ine {L}earning and an {A}pplication to {B}oosting}.
\bjtitle{Journal of Computer and System Sciences},
\bvolume{55},
\bfpage{119}--\bfpage{139}.
doi:10.1006/jcss.1997.1504.
\end{barticle}
\endbibitem

%%% 54
\bibitem{Tharwat2016}
\begin{barticle}
\bauthor{\bsnm{Tharwat}, \binits{A.}}
(\byear{2016}).
\batitle{{L}inear vs. quadratic discriminant analysis classifier: a tutorial}.
\bjtitle{Int. J. Appl. Pattern Recognit.},
\bvolume{3},
\bfpage{145}.
doi:10.1504/IJAPR.2016.079050.
\end{barticle}
\endbibitem

%%% 55
\bibitem{Cover1965}
\begin{barticle}
\bauthor{\bsnm{Cover}, \binits{Thomas M.}}
(\byear{1965}).
\batitle{{G}eometrical and statistical properties of systems of linear inequalities with applications in pattern recognition}.
\bjtitle{IEEE Trans. Electron. Comput.},
\bvolume{EC-14},
\bfpage{326}--\bfpage{334}.
doi:10.1109/PGEC.1965.264137.
\end{barticle}
\endbibitem

%%% 56
\bibitem{Ripley2009Mass}
\begin{barticle}
\bauthor{\bsnm{Ripley}, \binits{M.}}, \&
\bauthor{\bsnm{Venables}, \binits{B.}}
(\byear{2009}).
\batitle{{M}{A}{S}{S}: {S}upport {F}unctions and {D}atasets for {V}enables and {R}ipleyâ€™s {M}{A}{S}{S}}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/cran.package.mass.
\end{barticle}
\endbibitem

%%% 57
\bibitem{Ripley1996}
\begin{bbook}
\bauthor{\bsnm{Ripley}, \binits{B.D.}}
(\byear{1996}).
\bbtitle{{P}attern {R}ecognition and {N}eural {N}etworks}.
\blocation{Cambridge, United Kingdom}: 
\bpublisher{Cambridge University Press}.
doi:10.1017/CBO9780511812651.
\end{bbook}
\endbibitem

%%% 58
\bibitem{Venables2002}
\begin{bbook}
\bauthor{\bsnm{Venables}, \binits{B.}}, \&
\bauthor{\bsnm{Ripley}, \binits{B.}}
(\byear{2002}).
\bbtitle{{M}odern {A}pplied {S}tatistics {W}ith {S}}.
\blocation{Berlin/Heidelberg, Germany}: 
\bpublisher{Springer}.
doi:10.1007/b97626.
\end{bbook}
\endbibitem

%%% 59
\bibitem{rp5Description}
\begin{botherref}
\oauthor{\bsnm{rp5}}
(\byear{2024}).
{D}efinitions --- rp5.ru.
\url{https://rp5.ru/docs/descript/en}.
Accessed 30 November 2024.
\end{botherref}
\endbibitem

%%% 60
\bibitem{Ligges2006}
\begin{barticle}
\bauthor{\bsnm{Ligges}, \binits{U.}},
\bauthor{\bsnm{Short}, \binits{T.}}, \&
\bauthor{\bsnm{Kienzle}, \binits{P.}}
(\byear{2006}).
\batitle{signal: {S}ignal {P}rocessing}.
\bjtitle{CRAN: Contributed Packages}.
doi:10.32614/CRAN.package.signal.
\end{barticle}
\endbibitem

%%% 61
\bibitem{Benhamou2004}
\begin{barticle}
\bauthor{\bsnm{Benhamou}, \binits{S.}}
(\byear{2004}).
\batitle{{H}ow to reliably estimate the tortuosity of an animal's path: {S}traightness, sinuosity, or fractal dimension?}.
\bjtitle{Journal of theoretical biology},
\bvolume{229}.
doi:10.1016/j.jtbi.2004.03.016.
\end{barticle}
\endbibitem

%%% 62
\bibitem{Bovet1988}
\begin{barticle}
\bauthor{\bsnm{Bovet}, \binits{P.}}, \&
\bauthor{\bsnm{Benhamou}, \binits{S.}}
(\byear{1988}).
\batitle{{S}patial analysis of animals' movements using a correlated random walk model}.
\bjtitle{Journal of Theoretical Biology},
\bvolume{131},
\bfpage{419}--\bfpage{433}.
doi:10.1016/S0022-5193(88)80038-9.
\end{barticle}
\endbibitem

%%% 63
\bibitem{Cheung2007}
\begin{barticle}
\bauthor{\bsnm{Cheung}, \binits{A.}},
\bauthor{\bsnm{Zhang}, \binits{S.}},
\bauthor{\bsnm{Stricker}, \binits{C.}}, \&
\bauthor{\bsnm{Srinivasan}, \binits{M.V.}}
(\byear{2007}).
\batitle{{A}nimal navigation: the difficulty of moving in a straight line}.
\bjtitle{Biol Cybern},
\bvolume{97},
\bfpage{47}--\bfpage{61}.
doi:10.1007/s00422-007-0158-0.
\end{barticle}
\endbibitem

%%% 64
\bibitem{Mandelbrot1967}
\begin{bbook}
\bauthor{\bsnm{Mandelbrot}, \binits{B.}}
(\byear{1967}).
In \bbtitle{{H}ow {L}ong {I}s the {C}oast of {B}ritain? {S}tatistical {S}elf-{S}imilarity and {F}ractional {D}imension}
(pp. \bfpage{636}--\bfpage{638}).
\blocation{Washington, D.C., United States of America}: 
\bpublisher{American Association for the Advancement of Science}.
\end{bbook}
\endbibitem

%%% 65
\bibitem{Nams2006}
\begin{barticle}
\bauthor{\bsnm{Nams}, \binits{V.}}
(\byear{2006}).
\batitle{{I}mproving {A}ccuracy and {P}recision in {E}stimating {F}ractal {D}imension of {A}nimal movement paths}.
\bjtitle{Acta biotheoretica},
\bvolume{54},
\bfpage{1}--\bfpage{11}.
doi:10.1007/s10441-006-5954-8.
\end{barticle}
\endbibitem

%%% 66
\bibitem{Turchin1996}
\begin{barticle}
\bauthor{\bsnm{Turchin}, \binits{P.}}
(\byear{1996}).
\batitle{{F}ractal {A}nalyses of {A}nimal {M}ovement: {A} {C}ritique}.
\bjtitle{Ecology},
\bvolume{77},
\bfpage{2086}--\bfpage{2090}.
doi:10.2307/2265702.
\end{barticle}
\endbibitem

%%% 67
\bibitem{Kitamura2015}
\begin{barticle}
\bauthor{\bsnm{Kitamura}, \binits{T.}}, \&
\bauthor{\bsnm{Imafuku}, \binits{M.}}
(\byear{2015}).
\batitle{{B}ehavioural mimicry in flight path of {B}atesian intraspecific polymorphic butterfly {P}apilio polytes}.
\bjtitle{Proceedings. Biological sciences / The Royal Society},
\bvolume{282}.
doi:10.1098/rspb.2015.0483.
\end{barticle}
\endbibitem

%%% 68
\bibitem{altman1994diagnostic1}
\begin{barticle}
\bauthor{\bsnm{Altman}, \binits{D.G.}}, \&
\bauthor{\bsnm{Bland}, \binits{J.M.}}
(\byear{1994}).
\batitle{{D}iagnostic tests. 1: {S}ensitivity and specificity.}.
\bjtitle{BMJ: British Medical Journal},
\bvolume{308},
\bfpage{1552}.
doi:10.1136/bmj.308.6943.1552.
\end{barticle}
\endbibitem

%%% 69
\bibitem{altman1994diagnostic2}
\begin{barticle}
\bauthor{\bsnm{Altman}, \binits{D.G.}}, \&
\bauthor{\bsnm{Bland}, \binits{J.M.}}
(\byear{1994}).
\batitle{{D}iagnostic test 2: predictive values}.
\bjtitle{BMJ: British Medical Journal},
\bvolume{309},
\bfpage{102}.
doi:10.1136/bmj.309.6947.102.
\end{barticle}
\endbibitem

%%% 70
\bibitem{velez2007balanced}
\begin{barticle}
\bauthor{\bsnm{Velez}, \binits{D.R.}},
\bauthor{\bsnm{White}, \binits{B.C.}},
\bauthor{\bsnm{Motsinger}, \binits{Alison A.}},
\bauthor{\bsnm{Bush}, \binits{W.S.}},
\bauthor{\bsnm{Ritchie}, \binits{M.D.}},
\bauthor{\bsnm{Williams}, \binits{S.M.}}, \&
\bauthor{\bsnm{Moore}, \binits{J.H.}}
(\byear{2007}).
\batitle{{A} balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction}.
\bjtitle{Genetic Epidemiology: the Official Publication of the International Genetic Epidemiology Society},
\bvolume{31},
\bfpage{306}--\bfpage{315}.
doi:10.1002/gepi.20211.
\end{barticle}
\endbibitem

%%% 71
\bibitem{edwards1948note}
\begin{barticle}
\bauthor{\bsnm{Edwards}, \binits{A.L.}}
(\byear{1948}).
\batitle{{N}ote on the "correction for continuity" in testing the significance of the difference between correlated proportions}.
\bjtitle{Psychometrika},
\bvolume{13},
\bfpage{185}--\bfpage{187}.
doi:10.1007/BF02289261.
\end{barticle}
\endbibitem

%%% 72
\bibitem{bonferroni1936teoria}
\begin{bbook}
\bauthor{\bsnm{Bonferroni}, \binits{C.E.}}
(\byear{1936}).
\bbtitle{{T}eoria statistica delle classi e calcolo delle probabilit\`a}.
\blocation{Bagni di Lucca, Italy}: 
\bpublisher{Seeber}.
\end{bbook}
\endbibitem

%%% 73
\bibitem{mittelhammer2000econometric}
\begin{bbook}
\bauthor{\bsnm{Mittelhammer}, \binits{R.}},
\bauthor{\bsnm{Judge}, \binits{G.G.}}, \&
\bauthor{\bsnm{Miller}, \binits{D.J.}}
(\byear{2000}).
\bbtitle{{E}conometric foundations pack with {C}{D}-{R}{O}{M}}.
\blocation{Cambridge, United Kingdom}: 
\bpublisher{Cambridge University Press}.
\end{bbook}
\endbibitem

%%% 74
\bibitem{dunn1961multiple}
\begin{barticle}
\bauthor{\bsnm{Dunn}, \binits{O.J.}}
(\byear{1961}).
\batitle{{M}ultiple comparisons among means}.
\bjtitle{Journal of the American statistical association},
\bvolume{56},
\bfpage{52}--\bfpage{64}.
doi:10.2307/2282330.
\end{barticle}
\endbibitem

%%% 75
\bibitem{rupert2012simultaneous}
\begin{bbook}
\bauthor{\bsnm{Rupert Jr}, \binits{G.}}, \&
\bauthor{\bsnm{others}}
(\byear{2012}).
\bbtitle{{S}imultaneous statistical inference}.
\blocation{Berlin/Heidelberg, Germany}: 
\bpublisher{Springer Science \& Business Media}.
\end{bbook}
\endbibitem


\end{thebibliography}


\end{document}